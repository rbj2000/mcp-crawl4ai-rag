version: '3.8'

services:
  # Supabase deployment
  mcp-crawl4ai-supabase:
    build: 
      context: .
      args:
        VECTOR_DB_PROVIDER: supabase
    environment:
      - VECTOR_DB_PROVIDER=supabase
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MODEL_CHOICE=${MODEL_CHOICE}
      - USE_CONTEXTUAL_EMBEDDINGS=${USE_CONTEXTUAL_EMBEDDINGS:-false}
      - USE_HYBRID_SEARCH=${USE_HYBRID_SEARCH:-false}
      - USE_AGENTIC_RAG=${USE_AGENTIC_RAG:-false}
      - USE_RERANKING=${USE_RERANKING:-false}
      - USE_KNOWLEDGE_GRAPH=${USE_KNOWLEDGE_GRAPH:-false}
    ports:
      - "8051:8051"
    profiles: ["supabase"]

  # SQLite deployment (local development)
  mcp-crawl4ai-sqlite:
    build:
      context: .
      args:
        VECTOR_DB_PROVIDER: sqlite
    environment:
      - VECTOR_DB_PROVIDER=sqlite
      - SQLITE_DB_PATH=/data/vector_db.sqlite
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MODEL_CHOICE=${MODEL_CHOICE}
      - USE_CONTEXTUAL_EMBEDDINGS=${USE_CONTEXTUAL_EMBEDDINGS:-false}
      - USE_HYBRID_SEARCH=${USE_HYBRID_SEARCH:-false}
      - USE_AGENTIC_RAG=${USE_AGENTIC_RAG:-false}
      - USE_RERANKING=${USE_RERANKING:-false}
      - USE_KNOWLEDGE_GRAPH=${USE_KNOWLEDGE_GRAPH:-false}
    volumes:
      - ./data:/data
    ports:
      - "8052:8051"
    profiles: ["sqlite"]

  # Weaviate deployment
  mcp-crawl4ai-weaviate:
    build:
      context: .
      args:
        VECTOR_DB_PROVIDER: weaviate
    environment:
      - VECTOR_DB_PROVIDER=weaviate
      - WEAVIATE_URL=http://weaviate:8080
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MODEL_CHOICE=${MODEL_CHOICE}
      - USE_CONTEXTUAL_EMBEDDINGS=${USE_CONTEXTUAL_EMBEDDINGS:-false}
      - USE_HYBRID_SEARCH=${USE_HYBRID_SEARCH:-false}
      - USE_AGENTIC_RAG=${USE_AGENTIC_RAG:-false}
      - USE_RERANKING=${USE_RERANKING:-false}
      - USE_KNOWLEDGE_GRAPH=${USE_KNOWLEDGE_GRAPH:-false}
    depends_on:
      - weaviate
    ports:
      - "8053:8051"
    profiles: ["weaviate"]

  weaviate:
    image: semitechnologies/weaviate:latest
    environment:
      - QUERY_DEFAULTS_LIMIT=25
      - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate
      - ENABLE_MODULES=text2vec-openai
      - OPENAI_APIKEY=${OPENAI_API_KEY}
    ports:
      - "8080:8080"
    volumes:
      - weaviate_data:/var/lib/weaviate
    profiles: ["weaviate"]

  # Pinecone deployment
  mcp-crawl4ai-pinecone:
    build:
      context: .
      args:
        VECTOR_DB_PROVIDER: pinecone
    environment:
      - VECTOR_DB_PROVIDER=pinecone
      - PINECONE_API_KEY=${PINECONE_API_KEY}
      - PINECONE_ENVIRONMENT=${PINECONE_ENVIRONMENT}
      - PINECONE_INDEX_NAME=${PINECONE_INDEX_NAME}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MODEL_CHOICE=${MODEL_CHOICE}
      - USE_CONTEXTUAL_EMBEDDINGS=${USE_CONTEXTUAL_EMBEDDINGS:-false}
      - USE_HYBRID_SEARCH=${USE_HYBRID_SEARCH:-false}
      - USE_AGENTIC_RAG=${USE_AGENTIC_RAG:-false}
      - USE_RERANKING=${USE_RERANKING:-false}
      - USE_KNOWLEDGE_GRAPH=${USE_KNOWLEDGE_GRAPH:-false}
    ports:
      - "8054:8051"
    profiles: ["pinecone"]

  # Neo4j deployment (vector DB + knowledge graph)
  mcp-crawl4ai-neo4j:
    build:
      context: .
      args:
        VECTOR_DB_PROVIDER: all  # Include all dependencies since Neo4j already included
    environment:
      - VECTOR_DB_PROVIDER=neo4j_vector
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=neo4j_password
      - NEO4J_DATABASE=neo4j
      # Use same Neo4j instance for knowledge graph
      - KG_NEO4J_URI=bolt://neo4j:7687
      - KG_NEO4J_USER=neo4j
      - KG_NEO4J_PASSWORD=neo4j_password
      - KG_NEO4J_DATABASE=neo4j
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MODEL_CHOICE=${MODEL_CHOICE}
      - USE_CONTEXTUAL_EMBEDDINGS=${USE_CONTEXTUAL_EMBEDDINGS:-false}
      - USE_HYBRID_SEARCH=${USE_HYBRID_SEARCH:-true}  # Neo4j supports good hybrid search
      - USE_AGENTIC_RAG=${USE_AGENTIC_RAG:-false}
      - USE_RERANKING=${USE_RERANKING:-false}
      - USE_KNOWLEDGE_GRAPH=${USE_KNOWLEDGE_GRAPH:-true}  # Enable by default with Neo4j
    depends_on:
      neo4j:
        condition: service_healthy
    ports:
      - "8055:8051"
    profiles: ["neo4j"]

  neo4j:
    image: neo4j:5.15-community
    environment:
      - NEO4J_AUTH=neo4j/neo4j_password
      - NEO4J_PLUGINS=["apoc", "apoc-extended"]
      - NEO4J_db_tx__log_rotation_retention__policy=1 days
      # Enable vector search capabilities
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*,gds.*
    ports:
      - "7474:7474"  # Web interface
      - "7687:7687"  # Bolt protocol
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -u neo4j -p neo4j_password 'RETURN 1' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    profiles: ["neo4j"]

  # Ollama AI service deployment
  ollama:
    image: ollama/ollama:latest
    container_name: mcp-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    profiles: ["ollama", "ollama-full"]
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  # MCP with Ollama + SQLite (local development)
  mcp-crawl4ai-ollama-sqlite:
    build:
      context: .
      dockerfile: Dockerfile.multi-stage
      args:
        VECTOR_DB_PROVIDER: sqlite
        AI_PROVIDER: ollama
    environment:
      - VECTOR_DB_PROVIDER=sqlite
      - SQLITE_DB_PATH=/data/vector_db.sqlite
      - AI_PROVIDER=ollama
      - EMBEDDING_PROVIDER=ollama
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_EMBEDDING_MODEL=nomic-embed-text
      - OLLAMA_LLM_MODEL=llama3.2:1b
      - OLLAMA_EMBEDDING_DIMENSION=768
      - EMBEDDING_DIMENSION=768
      - MODEL_CHOICE=llama3.2:1b
      - USE_CONTEXTUAL_EMBEDDINGS=${USE_CONTEXTUAL_EMBEDDINGS:-true}
      - USE_HYBRID_SEARCH=${USE_HYBRID_SEARCH:-false}
      - USE_AGENTIC_RAG=${USE_AGENTIC_RAG:-true}
      - USE_RERANKING=${USE_RERANKING:-false}
      - USE_KNOWLEDGE_GRAPH=${USE_KNOWLEDGE_GRAPH:-false}
    volumes:
      - ./data:/data
    ports:
      - "8056:8051"
    depends_on:
      ollama:
        condition: service_healthy
    profiles: ["ollama", "ollama-sqlite"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8051/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # MCP with Ollama + Neo4j (full AI stack)
  mcp-crawl4ai-ollama-neo4j:
    build:
      context: .
      dockerfile: Dockerfile.multi-stage
      args:
        VECTOR_DB_PROVIDER: all
        AI_PROVIDER: ollama
    environment:
      - VECTOR_DB_PROVIDER=neo4j_vector
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=neo4j_password
      - NEO4J_DATABASE=neo4j
      - KG_NEO4J_URI=bolt://neo4j:7687
      - KG_NEO4J_USER=neo4j
      - KG_NEO4J_PASSWORD=neo4j_password
      - KG_NEO4J_DATABASE=neo4j
      - AI_PROVIDER=ollama
      - EMBEDDING_PROVIDER=ollama
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_EMBEDDING_MODEL=nomic-embed-text
      - OLLAMA_LLM_MODEL=llama3.2:1b
      - OLLAMA_EMBEDDING_DIMENSION=768
      - EMBEDDING_DIMENSION=768
      - MODEL_CHOICE=llama3.2:1b
      - USE_CONTEXTUAL_EMBEDDINGS=${USE_CONTEXTUAL_EMBEDDINGS:-true}
      - USE_HYBRID_SEARCH=${USE_HYBRID_SEARCH:-true}
      - USE_AGENTIC_RAG=${USE_AGENTIC_RAG:-true}
      - USE_RERANKING=${USE_RERANKING:-false}
      - USE_KNOWLEDGE_GRAPH=${USE_KNOWLEDGE_GRAPH:-true}
    depends_on:
      neo4j:
        condition: service_healthy
      ollama:
        condition: service_healthy
    ports:
      - "8057:8051"
    profiles: ["ollama-full"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8051/health || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 180s

  # Hybrid OpenAI/Ollama deployment (OpenAI embeddings, Ollama LLM)
  mcp-crawl4ai-hybrid:
    build:
      context: .
      dockerfile: Dockerfile.multi-stage
      args:
        VECTOR_DB_PROVIDER: supabase
        AI_PROVIDER: mixed
    environment:
      - VECTOR_DB_PROVIDER=supabase
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - AI_PROVIDER=mixed
      - EMBEDDING_PROVIDER=openai
      - LLM_PROVIDER=ollama
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_LLM_MODEL=llama3.2:1b
      - EMBEDDING_DIMENSION=1536
      - MODEL_CHOICE=llama3.2:1b
      - USE_CONTEXTUAL_EMBEDDINGS=${USE_CONTEXTUAL_EMBEDDINGS:-true}
      - USE_HYBRID_SEARCH=${USE_HYBRID_SEARCH:-true}
      - USE_AGENTIC_RAG=${USE_AGENTIC_RAG:-true}
      - USE_RERANKING=${USE_RERANKING:-false}
      - USE_KNOWLEDGE_GRAPH=${USE_KNOWLEDGE_GRAPH:-false}
    depends_on:
      ollama:
        condition: service_healthy
    ports:
      - "8058:8051"
    profiles: ["hybrid"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8051/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Monitoring stack
  prometheus:
    image: prom/prometheus:latest
    container_name: mcp-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./deployment/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    profiles: ["monitoring", "ollama-full"]

  grafana:
    image: grafana/grafana:latest
    container_name: mcp-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./deployment/monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./deployment/monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    profiles: ["monitoring", "ollama-full"]

  # Model initialization service
  ollama-init:
    image: ollama/ollama:latest
    container_name: mcp-ollama-init
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      ollama:
        condition: service_healthy
    command: >
      sh -c "
        echo 'Pulling required models...' &&
        ollama pull nomic-embed-text &&
        ollama pull llama3.2:1b &&
        echo 'Models ready!' &&
        ollama list
      "
    profiles: ["ollama", "ollama-full", "hybrid"]
    restart: "no"

volumes:
  weaviate_data:
  neo4j_data:
  neo4j_logs:
  ollama_models:
  prometheus_data:
  grafana_data: