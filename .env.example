# MCP Server Configuration
HOST=0.0.0.0
PORT=8051
TRANSPORT=sse

# ==============================================================================
# AI Provider Configuration
# ==============================================================================
# Choose your AI provider: openai, ollama, vllm, or mixed (separate embedding/LLM)
AI_PROVIDER=openai

# Or configure providers separately for cost optimization:
# EMBEDDING_PROVIDER=vllm
# LLM_PROVIDER=openai

# Legacy configuration (backward compatible)
MODEL_CHOICE=gpt-4o-mini

# ------------------------------------------------------------------------------
# OpenAI Configuration (when AI_PROVIDER=openai or LLM_PROVIDER=openai)
# ------------------------------------------------------------------------------
OPENAI_API_KEY=your_openai_api_key
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_LLM_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_DIMENSIONS=1536

# ------------------------------------------------------------------------------
# Ollama Configuration (when AI_PROVIDER=ollama or *_PROVIDER=ollama)
# ------------------------------------------------------------------------------
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_LLM_MODEL=llama3.2:1b
OLLAMA_EMBEDDING_DIMENSIONS=768

# ------------------------------------------------------------------------------
# vLLM Configuration (when AI_PROVIDER=vllm or *_PROVIDER=vllm)
# ------------------------------------------------------------------------------
# vLLM Provider - Cloud-deployed vLLM for text, embeddings, and vision models
# Supports OpenAI-compatible API endpoints

# vLLM Endpoint Configuration
VLLM_BASE_URL=https://your-vllm-endpoint.com/v1
VLLM_API_KEY=your_vllm_api_key

# Text and Embedding Models
VLLM_TEXT_MODEL=meta-llama/Llama-3.1-8B-Instruct
VLLM_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
VLLM_EMBEDDING_DIMENSIONS=1024

# Vision Model Configuration (for multi-modal RAG)
VLLM_VISION_MODEL=llava-hf/llava-v1.6-mistral-7b-hf
VLLM_VISION_ENABLED=true

# Performance Tuning
VLLM_MAX_BATCH_SIZE=32
VLLM_MAX_RETRIES=3
VLLM_TIMEOUT=120.0

# Supported vLLM Text Models:
# - meta-llama/Llama-3.1-8B-Instruct (default)
# - meta-llama/Llama-3.1-70B-Instruct
# - mistralai/Mistral-7B-Instruct-v0.3
# - mistralai/Mixtral-8x7B-Instruct-v0.1

# Supported vLLM Embedding Models:
# - BAAI/bge-large-en-v1.5 (1024 dims) (default)
# - BAAI/bge-base-en-v1.5 (768 dims)
# - sentence-transformers/all-MiniLM-L6-v2 (384 dims)

# Supported vLLM Vision Models:
# - llava-hf/llava-v1.6-mistral-7b-hf (default)
# - llava-hf/llava-v1.6-vicuna-7b-hf
# - Qwen/Qwen2-VL-7B-Instruct
# - microsoft/Phi-3-vision-128k-instruct

# RAG Strategies (set to "true" or "false", default to "false")
USE_CONTEXTUAL_EMBEDDINGS=false
USE_HYBRID_SEARCH=false
USE_AGENTIC_RAG=false
USE_RERANKING=false
USE_KNOWLEDGE_GRAPH=false

# Reranking Configuration (when USE_RERANKING=true)
RERANKING_PROVIDER=ollama  # ollama, openai, huggingface
RERANKING_MODEL=bge-reranker-base  # provider-specific model (see docs for options)
RERANKING_MAX_RESULTS=100  # maximum number of results to rerank
RERANKING_TIMEOUT=30.0  # timeout in seconds for reranking operations

# HuggingFace Configuration (when RERANKING_PROVIDER=huggingface)
HUGGINGFACE_TOKEN=your_huggingface_token

# Database Provider Selection
VECTOR_DB_PROVIDER=supabase  # supabase, pinecone, weaviate, sqlite, neo4j_vector

# Supabase Configuration (when VECTOR_DB_PROVIDER=supabase)
SUPABASE_URL=your_supabase_project_url
SUPABASE_SERVICE_KEY=your_supabase_service_key

# Pinecone Configuration (when VECTOR_DB_PROVIDER=pinecone)
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_environment
PINECONE_INDEX_NAME=crawl4ai-rag

# Weaviate Configuration (when VECTOR_DB_PROVIDER=weaviate)
WEAVIATE_URL=your_weaviate_url
WEAVIATE_API_KEY=your_weaviate_api_key
WEAVIATE_CLASS_NAME=Document

# SQLite Configuration (when VECTOR_DB_PROVIDER=sqlite)
SQLITE_DB_PATH=./vector_db.sqlite
EMBEDDING_DIMENSION=1536

# Neo4j Vector Configuration (when VECTOR_DB_PROVIDER=neo4j_vector)
# This is for using Neo4j as your primary vector database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password
NEO4J_DATABASE=neo4j

# Knowledge Graph Neo4j Configuration (separate from vector DB)
# Required for hallucination detection features - can be same or different Neo4j instance
KG_NEO4J_URI=bolt://localhost:7687
KG_NEO4J_USER=neo4j
KG_NEO4J_PASSWORD=your_neo4j_password
KG_NEO4J_DATABASE=neo4j

# NOTE: You can use the same Neo4j instance for both vector storage and knowledge graphs
# by setting the same connection details for both. They use different node labels:
# - Vector DB: Document, CodeExample, Source nodes
# - Knowledge Graph: Repository, File, Class, Function, Method nodes

# ─── Confluence Integration ─────────────────────────────────────────
# Base URL of your Confluence instance
CONFLUENCE_URL=https://your-org.atlassian.net

# Deployment type (auto-detected from URL if not set): cloud, on_prem, data_center
# CONFLUENCE_DEPLOYMENT_TYPE=cloud

# Auth method (auto-detected from available credentials if not set): basic, pat, oauth2
# CONFLUENCE_AUTH_METHOD=basic

# Basic Auth (Cloud) — email + API token
CONFLUENCE_USERNAME=your_email@example.com
CONFLUENCE_API_TOKEN=your_confluence_api_token

# Personal Access Token (On-Prem / Data Center)
# CONFLUENCE_PAT=your_personal_access_token

# OAuth 2.0 (Cloud Enterprise) — run scripts/confluence_oauth_setup.py to obtain tokens
# CONFLUENCE_OAUTH2_CLIENT_ID=your_oauth2_client_id
# CONFLUENCE_OAUTH2_CLIENT_SECRET=your_oauth2_client_secret
# CONFLUENCE_OAUTH2_ACCESS_TOKEN=your_access_token
# CONFLUENCE_OAUTH2_REFRESH_TOKEN=your_refresh_token
# CONFLUENCE_OAUTH2_TOKEN_EXPIRES_AT=unix_timestamp
# CONFLUENCE_CLOUD_ID=your_cloud_id

# SSL Configuration
# CONFLUENCE_SSL_VERIFY=true
# CONFLUENCE_CA_BUNDLE=/path/to/ca-bundle.pem

# Connection tuning
# CONFLUENCE_TIMEOUT=30.0
# CONFLUENCE_CONNECT_TIMEOUT=10.0
# CONFLUENCE_POOL_SIZE=10
# CONFLUENCE_MAX_RETRIES=3