# Hybrid Environment Configuration
# =================================
# OpenAI embeddings + Ollama LLM for cost optimization

# Server Configuration
HOST=0.0.0.0
PORT=8051
TRANSPORT=sse
ENVIRONMENT=hybrid

# AI Provider Configuration (Mixed providers)
AI_PROVIDER=mixed
EMBEDDING_PROVIDER=openai
LLM_PROVIDER=ollama

# OpenAI Configuration (For embeddings only)
# OPENAI_API_KEY=your_openai_api_key
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSION=1536

# Ollama Configuration (For LLM only)
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_LLM_MODEL=llama3.2:1b
MODEL_CHOICE=llama3.2:1b

# Ollama Performance Settings
OLLAMA_NUM_PARALLEL=2
OLLAMA_NUM_THREAD=8
OLLAMA_CONTEXT_SIZE=4096
OLLAMA_KEEP_ALIVE=5m

# Vector Database Configuration (Supabase for scalability)
VECTOR_DB_PROVIDER=supabase
# SUPABASE_URL=your_supabase_url
# SUPABASE_SERVICE_KEY=your_supabase_key

# RAG Strategy Configuration (Balanced approach)
USE_CONTEXTUAL_EMBEDDINGS=true
USE_HYBRID_SEARCH=true
USE_AGENTIC_RAG=true
USE_RERANKING=false
USE_KNOWLEDGE_GRAPH=false

# Performance Configuration (Balanced)
MAX_CONCURRENT_REQUESTS=10
BATCH_SIZE=25
CACHE_TTL=1800  # 30 minutes
EMBEDDINGS_BATCH_SIZE=15
QUERY_TIMEOUT=45  # Account for Ollama processing time

# Security Configuration
CORS_ORIGINS=https://yourdomain.com
RATE_LIMIT_PER_MINUTE=45
MAX_CONTENT_LENGTH=8388608  # 8MB

# Logging Configuration
LOG_LEVEL=INFO
LOG_FORMAT=json
ENABLE_METRICS=true
PROMETHEUS_PORT=9090

# Provider-specific Health Checks
OPENAI_HEALTH_CHECK_ENABLED=true
OLLAMA_HEALTH_CHECK_ENABLED=true
OLLAMA_HEALTH_CHECK_TIMEOUT=20

# Docker Compose Profile
COMPOSE_PROFILES=hybrid

# Cost Optimization Settings
ENABLE_EMBEDDING_CACHE=true
EMBEDDING_CACHE_TTL=86400  # 24 hours
LLM_RESPONSE_CACHE_TTL=3600  # 1 hour
OPTIMIZE_FOR_COST=true

# Feature Flags
ENABLE_DEBUG_ENDPOINTS=false
ENABLE_SWAGGER_UI=true
ENABLE_PROFILING=false