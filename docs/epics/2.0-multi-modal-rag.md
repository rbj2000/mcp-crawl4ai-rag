# Epic 2.0: Multi-Modal RAG - Images, Code, and Text Integration

## Status
Planning

## Epic Goal

Enable users to search and retrieve documentation that contains screenshots, diagrams, and enhanced code snippets through unified multi-modal RAG capabilities, leveraging vLLM for both text and vision model inference.

## Business Value

**Problem Statement:**
Users need to search technical documentation that contains visual elements (screenshots, architecture diagrams, UI mockups) and code examples, but current text-only RAG cannot effectively index or retrieve this content. This limits the usefulness of the system for modern documentation that relies heavily on visual communication.

**Value Proposition:**
- **Enhanced Search Quality**: Find relevant documentation even when key information is in images
- **Complete Context**: Retrieve diagrams and screenshots alongside related text
- **Code Understanding**: Better semantic understanding of code through enhanced processing
- **Unified Experience**: Single query interface for text, images, and code

**Success Metrics:**
- Multi-modal queries return relevant results including images within 2 seconds
- Image-based search recall improves by 40%+ for documentation with visual content
- User satisfaction with search results increases measurably

## Existing System Context

### Current Capabilities
- **Text RAG**: Markdown content processing, semantic chunking, vector embeddings
- **AI Providers**: OpenAI, Ollama, HuggingFace with provider abstraction
- **Vector Databases**: Supabase, SQLite, Neo4j, Pinecone, Weaviate
- **RAG Strategies**: Contextual embeddings, hybrid search, reranking, knowledge graphs
- **Code Processing**: Basic code extraction and text-based indexing

### Technology Stack
- **Runtime**: Python 3.13, FastMCP server
- **AI Provider Layer**: Abstract provider interfaces with factory pattern
- **Database Layer**: Provider-agnostic vector storage
- **Web Crawler**: Crawl4AI with browser automation
- **Current Deployment**: Docker orchestration with multiple provider profiles

### Integration Points
1. **AI Provider System**: `src/ai_providers/` - Add vLLM provider
2. **Crawler Pipeline**: `src/crawl4ai_mcp.py` - Extend for image extraction
3. **Vector Storage**: `src/database/providers/` - Schema updates for image metadata
4. **RAG Query**: `perform_rag_query()` - Multi-modal retrieval logic
5. **Embedding Generation**: `src/utils_refactored.py` - Image embedding support

## Enhancement Details

### What's Being Added

**1. vLLM AI Provider Integration**
- New vLLM provider supporting text and vision models
- OpenAI-compatible API client for vLLM endpoints
- Support for models like LLaVA, Qwen-VL, Phi-3-Vision
- Cloud-deployed vLLM instances accessed via API

**2. Image Processing Pipeline**
- Image extraction during web crawling (screenshots, diagrams, charts)
- Image storage strategy (blob storage or file references)
- Metadata extraction (dimensions, format, alt text, context)
- Image-to-text descriptions via vision models

**3. Vision Model Capabilities**
- Image embedding generation for semantic similarity
- Image captioning and description
- OCR for text extraction from images
- Visual question answering for image content

**4. Enhanced Code Understanding**
- Improved code snippet extraction and context preservation
- Code syntax highlighting and rendering
- Enhanced metadata (language, complexity, dependencies)
- Code-to-image conversion for visual code search (optional)

**5. Multi-Modal Retrieval**
- Unified search: text query → relevant text + images
- Image similarity search
- Cross-modal ranking and reranking
- Context-aware result presentation

**6. Database Schema Enhancements**
- New tables/fields for image metadata
- Image embedding vector storage
- Relationship tracking (image ↔ text content)
- Migration path for existing data

### How It Integrates

**Provider Architecture Extension:**
```
Existing:  BaseProvider → EmbeddingProvider → OllamaProvider/OpenAIProvider
New:       BaseProvider → VisionProvider → vLLMProvider
           ↳ Implements both EmbeddingProvider and VisionProvider interfaces
```

**Crawl Pipeline Enhancement:**
```
1. Web Crawl → Extract HTML/Markdown (existing)
2. Image Discovery → Download & Store Images (new)
3. Vision Model Processing → Generate embeddings + descriptions (new)
4. Unified Storage → Store text + image vectors (enhanced)
```

**Query Flow Enhancement:**
```
User Query →
  ├─ Text Embedding (existing)
  ├─ Vector Search → Text Results (existing)
  ├─ Vector Search → Image Results (new)
  ├─ Multi-Modal Reranking (new)
  └─ Unified Response (enhanced)
```

### Success Criteria

**Functional Requirements:**
- ✅ vLLM provider successfully integrated with text and vision model support
- ✅ Images extracted and stored during crawling with metadata
- ✅ Image embeddings generated and searchable via vector similarity
- ✅ Text queries return relevant images alongside text results
- ✅ Image descriptions/captions generated for accessibility and search
- ✅ Enhanced code snippets properly extracted and indexed
- ✅ Multi-modal search results properly ranked and presented

**Quality Requirements:**
- ✅ No regression in existing text-only RAG functionality
- ✅ Image processing adds < 30% overhead to crawl time
- ✅ Multi-modal queries complete within 2 seconds (p95)
- ✅ Image search precision > 70% for relevant queries
- ✅ System remains provider-agnostic (fallback to text-only if vision unavailable)

**Compatibility Requirements:**
- ✅ Existing MCP tools continue to work unchanged for text-only use cases
- ✅ Database schema changes backward compatible
- ✅ Configuration follows existing environment variable patterns
- ✅ Existing AI provider configurations remain valid

## Epic Stories

### Story 2.1: vLLM AI Provider Integration
**As a** system administrator,
**I want** vLLM support integrated as a new AI provider,
**so that** I can use cloud-deployed vLLM instances for text and vision model inference.

**Scope:**
- Create `vLLMProvider` class implementing text and vision interfaces
- OpenAI-compatible API client configuration
- Health checks and error handling
- Configuration via environment variables (VLLM_BASE_URL, VLLM_API_KEY, etc.)
- Integration with existing provider factory

**Acceptance Criteria:**
- vLLM provider creates embeddings for text
- vLLM provider generates image embeddings via vision models
- Provider factory successfully creates vLLM instances
- Health checks validate vLLM endpoint connectivity
- Documentation updated with vLLM configuration examples

---

### Story 2.2: Image Extraction and Storage Infrastructure
**As a** developer using the crawler,
**I want** images automatically extracted and stored during web crawling,
**so that** visual content is available for multi-modal search.

**Scope:**
- Extend `crawl_single_page` to extract images from HTML
- Image download and local/blob storage implementation
- Metadata extraction (dimensions, format, alt text, surrounding context)
- Database schema updates for image metadata table
- Image-to-document relationship tracking

**Acceptance Criteria:**
- Images extracted from crawled pages with metadata
- Images stored efficiently (configurable: filesystem vs. blob storage)
- Database schema supports image metadata storage
- Image URLs and local paths properly tracked
- No breaking changes to existing crawl functionality

---

### Story 2.3: Vision Model Integration for Image Understanding
**As a** user searching documentation,
**I want** images to have semantic descriptions and embeddings,
**so that** I can find relevant images using natural language queries.

**Scope:**
- Image embedding generation via vLLM vision models
- Image captioning/description generation
- OCR text extraction from images (for text-in-images)
- Vision model configuration and fallback handling
- Integration with existing embedding pipeline

**Acceptance Criteria:**
- Images have vector embeddings stored in vector database
- Image descriptions automatically generated and indexed
- OCR extracts text from screenshots/diagrams
- Vision model failures gracefully degrade to metadata-only indexing
- Performance metrics logged for vision model calls

---

### Story 2.4: Enhanced Code Snippet Processing
**As a** developer searching code examples,
**I want** better code understanding and extraction,
**so that** I can find relevant code snippets more accurately.

**Scope:**
- Enhanced code block extraction with language detection
- Syntax-aware metadata extraction (imports, classes, functions)
- Code context preservation (surrounding documentation)
- Integration with existing code examples table (if USE_AGENTIC_RAG)
- Code highlighting and formatting preservation

**Acceptance Criteria:**
- Code snippets extracted with accurate language detection
- Code metadata (complexity, dependencies) captured
- Code context (surrounding text) preserved for better relevance
- Enhanced code search quality measurably improved
- Backward compatible with existing code extraction

---

### Story 2.5: Multi-Modal RAG Query Implementation
**As a** user of the RAG system,
**I want** to search across text, images, and code with a single query,
**so that** I get comprehensive results including visual content.

**Scope:**
- Extend `perform_rag_query` for multi-modal retrieval
- Text query → search both text and image vectors
- Cross-modal ranking (relevance scoring across modalities)
- Result presentation with text + image results
- Configuration flag for enabling/disabling multi-modal search

**Acceptance Criteria:**
- Single query returns relevant text and image results
- Results properly ranked across modalities
- MCP tool `perform_rag_query` returns image URLs/references
- Multi-modal search configurable via USE_MULTIMODAL_RAG flag
- Performance metrics for multi-modal queries logged

---

### Story 2.6: Multi-Modal Reranking and Result Fusion
**As a** user receiving search results,
**I want** results intelligently ranked across text and images,
**so that** the most relevant content appears first regardless of modality.

**Scope:**
- Multi-modal reranking strategy (text + image fusion)
- Cross-modal relevance scoring
- Integration with existing reranking providers (Story 1.1)
- Configurable ranking weights (text vs. image importance)
- Result deduplication and clustering

**Acceptance Criteria:**
- Multi-modal results properly ranked by relevance
- Reranking respects provider configuration (Ollama, OpenAI, vLLM)
- Users can configure text/image ranking weights
- Result quality measurably improved over simple concatenation
- No performance regression for text-only queries

---

### Story 2.7: Testing, Documentation, and Validation
**As a** developer and user of the system,
**I want** comprehensive testing and documentation for multi-modal features,
**so that** I can confidently deploy and use the enhanced system.

**Scope:**
- Unit tests for vLLM provider and image processing
- Integration tests for end-to-end multi-modal RAG
- Performance benchmarks (crawl time, query latency, accuracy)
- Documentation updates (CLAUDE.md, configuration examples)
- Migration guide for existing deployments
- Example queries and expected results

**Acceptance Criteria:**
- Test coverage > 80% for new code
- Integration tests validate full multi-modal workflow
- Performance benchmarks documented (baseline vs. multi-modal)
- CLAUDE.md updated with vLLM and multi-modal configuration
- Migration guide for database schema updates
- Example documentation site with images crawled and searchable

## Technical Approach

### Architecture Principles
- **Provider Abstraction**: vLLM follows existing provider patterns
- **Graceful Degradation**: System works without vision models (text-only fallback)
- **Configuration-Driven**: All features controlled via environment variables
- **Backward Compatibility**: Existing deployments unaffected unless explicitly enabled
- **Performance First**: Image processing optimized for minimal latency impact

### Key Design Decisions

**1. vLLM as Primary Vision Provider**
- Cloud-deployed vLLM offers performance and cost benefits
- OpenAI-compatible API simplifies integration
- Supports multiple vision models (LLaVA, Qwen-VL, etc.)

**2. Image Storage Strategy**
- Configurable: filesystem (development) vs. blob storage (production)
- Image embeddings always in vector database
- Metadata includes storage location references

**3. Multi-Modal Retrieval Approach**
- Separate vector searches for text and images
- Results merged and reranked using cross-modal scoring
- Existing reranking providers extended for multi-modal support

**4. Backward Compatibility**
- New configuration flag: `USE_MULTIMODAL_RAG=true/false`
- Database schema changes additive (no breaking changes)
- Existing MCP tools work unchanged when multi-modal disabled

### Configuration Examples

```bash
# vLLM Provider Configuration
AI_PROVIDER=vllm
VLLM_BASE_URL=https://your-vllm-endpoint.com/v1
VLLM_API_KEY=your_vllm_api_key
VLLM_TEXT_MODEL=meta-llama/Llama-3.1-8B-Instruct
VLLM_VISION_MODEL=llava-hf/llava-v1.6-mistral-7b-hf
VLLM_EMBEDDING_DIMENSIONS=768

# Multi-Modal RAG Configuration
USE_MULTIMODAL_RAG=true
IMAGE_STORAGE_TYPE=filesystem  # or blob_storage
IMAGE_STORAGE_PATH=/app/data/images
EXTRACT_IMAGE_TEXT=true  # Enable OCR
GENERATE_IMAGE_CAPTIONS=true

# Multi-Modal Reranking
USE_RERANKING=true
RERANKING_PROVIDER=vllm
MULTIMODAL_RANKING_TEXT_WEIGHT=0.6
MULTIMODAL_RANKING_IMAGE_WEIGHT=0.4

# Existing providers continue to work
# AI_PROVIDER=ollama  # Fallback to text-only
# AI_PROVIDER=openai  # Can use GPT-4V for vision if available
```

## Risk Mitigation

### Primary Risks

**1. Performance Impact**
- **Risk**: Image processing significantly slows crawling
- **Mitigation**:
  - Async image processing pipeline
  - Configurable image size limits
  - Optional image processing (can disable per crawl)
  - Performance benchmarks in Story 2.7
- **Rollback**: Disable via `USE_MULTIMODAL_RAG=false`

**2. Storage Growth**
- **Risk**: Image storage significantly increases disk usage
- **Mitigation**:
  - Configurable image storage limits
  - Image compression and resizing options
  - Cleanup policies for old/unused images
  - Blob storage support for scalable storage
- **Monitoring**: Storage metrics in health checks

**3. Vision Model Availability**
- **Risk**: vLLM endpoint unavailable breaks system
- **Mitigation**:
  - Graceful fallback to text-only processing
  - Retry logic with exponential backoff
  - Health checks with alerting
  - Multiple vLLM endpoint support (failover)
- **Rollback**: System degrades to text-only mode

**4. Database Schema Migration**
- **Risk**: Schema changes break existing deployments
- **Mitigation**:
  - Additive schema changes only (no breaking modifications)
  - Migration scripts provided
  - Schema versioning
  - Backward compatibility validation
- **Rollback**: Schema supports both old and new formats

**5. Integration Complexity**
- **Risk**: Multi-modal features conflict with existing RAG strategies
- **Mitigation**:
  - Feature flag isolation (`USE_MULTIMODAL_RAG`)
  - Comprehensive integration testing (Story 2.7)
  - Existing functionality regression tests
  - Phased rollout approach (enable per-source)

## Compatibility Requirements

- ✅ **Existing APIs Unchanged**: MCP tools maintain existing signatures
- ✅ **Database Backward Compatible**: Schema changes are additive
- ✅ **Configuration Backward Compatible**: Existing .env files work unchanged
- ✅ **Provider Independence**: System works with any provider (with/without vision)
- ✅ **Performance Neutral**: Text-only queries unaffected by multi-modal features
- ✅ **Graceful Degradation**: System works without vision models enabled

## Definition of Done

Epic is complete when:

- ✅ All 7 stories completed with acceptance criteria met
- ✅ vLLM provider fully integrated and tested
- ✅ Images extracted, stored, and searchable
- ✅ Multi-modal queries return relevant text + image results
- ✅ No regression in existing text-only RAG functionality
- ✅ Performance benchmarks meet success criteria (<2s p95 latency)
- ✅ Documentation comprehensive (configuration, migration, examples)
- ✅ Test coverage > 80% for new code
- ✅ Example deployment with multi-modal search demonstrated
- ✅ Production-ready with monitoring and health checks

## Dependencies and Sequencing

**Story Dependencies:**
```
2.1 (vLLM Provider) → 2.3 (Vision Models)
2.2 (Image Storage) → 2.3 (Vision Models)
2.3 (Vision Models) → 2.5 (Multi-Modal Query)
2.4 (Code Processing) → 2.5 (Multi-Modal Query)
2.5 (Multi-Modal Query) → 2.6 (Reranking)
All Stories → 2.7 (Testing & Documentation)
```

**Recommended Implementation Order:**
1. **Story 2.1** - Foundation: vLLM provider
2. **Story 2.2** - Infrastructure: Image storage
3. **Story 2.3** - Core capability: Vision models
4. **Story 2.4** - Parallel: Enhanced code (can be done alongside 2.3)
5. **Story 2.5** - Integration: Multi-modal query
6. **Story 2.6** - Enhancement: Reranking
7. **Story 2.7** - Validation: Testing & docs

**External Dependencies:**
- vLLM deployment endpoint (cloud-hosted)
- Vision models available on vLLM (LLaVA, Qwen-VL, etc.)
- Sufficient storage for images (filesystem or blob storage)
- Database provider with vector support (existing)

## Success Validation

**Functional Validation:**
- Crawl a documentation site with images → images extracted and stored
- Query "architecture diagram" → relevant images returned
- Query "code example for authentication" → code + related diagrams returned
- Vision model failure → system continues with text-only mode

**Performance Validation:**
- Crawl performance: < 30% overhead with image processing
- Query latency: < 2 seconds p95 for multi-modal queries
- Image search precision: > 70% for relevant queries
- Storage efficiency: Reasonable disk usage (configurable limits work)

**Quality Validation:**
- User testing: Search quality improves for documentation with visuals
- Regression testing: All existing tests pass
- Integration testing: Full end-to-end multi-modal workflow works
- Documentation: Clear migration and configuration guides

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-30 | 1.0 | Initial epic creation for Multi-Modal RAG | John (PM) |

## Notes

- This epic extends the foundation laid by Epic 1.0 (Provider-Agnostic Reranking)
- vLLM provider architecture mirrors existing Ollama/OpenAI patterns
- Multi-modal features are opt-in via configuration flags
- Production deployment requires vLLM cloud endpoint setup
- Consider starting with Story 2.1 to validate vLLM integration early