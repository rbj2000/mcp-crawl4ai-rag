# Story 2.6: Multi-Modal Reranking and Result Fusion

## Status
Draft

## Story
**As a** user receiving search results,
**I want** results intelligently ranked across text and images,
**so that** the most relevant content appears first regardless of modality.

## Acceptance Criteria

### Functional Requirements
1. Multi-modal results properly ranked by relevance
2. Reranking respects provider configuration (Ollama, OpenAI, vLLM, HuggingFace)
3. Users can configure text/image/code ranking weights
4. Result quality measurably improved over simple score-based concatenation
5. Cross-modal relevance scoring (e.g., image relevance to text query)

### Integration Requirements
6. Integration with existing reranking providers from Story 1.1
7. Extension of reranking interfaces for multi-modal support
8. Configuration via environment variables (MULTIMODAL_RANKING_TEXT_WEIGHT, etc.)
9. Backward compatible with text-only reranking

### Quality Requirements
10. No performance regression for text-only queries
11. Multi-modal reranking adds < 500ms to query latency
12. Comprehensive tests for multi-modal reranking
13. Documentation updated with multi-modal reranking configuration

## Tasks / Subtasks

- [ ] **Extend Reranking Provider Interfaces** (AC: 2, 6, 7)
  - [ ] Add `MultiModalRerankingProvider` interface to `src/ai_providers/base.py`
  - [ ] Add `rerank_multimodal()` method to interface
  - [ ] Extend existing reranking providers (Ollama, OpenAI, vLLM, HuggingFace)
  - [ ] Implement multi-modal reranking for each provider
  - [ ] Add fallback to single-modality reranking if multi-modal not supported

- [ ] **Implement Cross-Modal Relevance Scoring** (AC: 5)
  - [ ] Design cross-modal scoring algorithm
  - [ ] Text-to-image relevance scoring
  - [ ] Image-to-text relevance scoring
  - [ ] Code-to-text/image relevance scoring
  - [ ] Implement scoring via LLM prompts (for capable providers)
  - [ ] Implement scoring via embedding similarity (fallback)

- [ ] **Implement Configurable Ranking Weights** (AC: 3)
  - [ ] Add weight configuration (text, image, code weights)
  - [ ] Apply weights to modality scores
  - [ ] Normalize weighted scores
  - [ ] Support dynamic weight adjustment per query
  - [ ] Implement weight validation (sum to 1.0)

- [ ] **Ollama Multi-Modal Reranking** (AC: 2, 6)
  - [ ] Implement multi-modal reranking for Ollama provider
  - [ ] Use LLM to score text-image relevance
  - [ ] Design prompts for cross-modal scoring
  - [ ] Handle vision models if available (llava, etc.)
  - [ ] Fallback to embedding similarity if vision unavailable

- [ ] **OpenAI Multi-Modal Reranking** (AC: 2, 6)
  - [ ] Implement multi-modal reranking for OpenAI provider
  - [ ] Use GPT-4V for cross-modal relevance if available
  - [ ] Use embedding similarity as fallback
  - [ ] Handle rate limits and costs
  - [ ] Batch multiple reranking requests

- [ ] **vLLM Multi-Modal Reranking** (AC: 2, 6)
  - [ ] Implement multi-modal reranking for vLLM provider
  - [ ] Use vision models for cross-modal scoring
  - [ ] Optimize for vLLM endpoint performance
  - [ ] Handle vision model availability
  - [ ] Implement caching for repeated queries

- [ ] **HuggingFace Multi-Modal Reranking** (AC: 2, 6)
  - [ ] Implement multi-modal reranking for HuggingFace provider
  - [ ] Use CLIP or similar multi-modal models
  - [ ] Implement cross-modal embedding similarity
  - [ ] Handle model loading and caching
  - [ ] Fallback to text-only reranking

- [ ] **Result Fusion Strategy** (AC: 1, 4)
  - [ ] Implement reciprocal rank fusion (RRF)
  - [ ] Implement weighted score fusion
  - [ ] Implement LLM-based fusion (re-score all results together)
  - [ ] Compare fusion strategies for quality
  - [ ] Add configurable fusion strategy selection

- [ ] **Configuration System** (AC: 3, 8)
  - [ ] Add `MULTIMODAL_RANKING_TEXT_WEIGHT` (default: 0.5)
  - [ ] Add `MULTIMODAL_RANKING_IMAGE_WEIGHT` (default: 0.3)
  - [ ] Add `MULTIMODAL_RANKING_CODE_WEIGHT` (default: 0.2)
  - [ ] Add `MULTIMODAL_FUSION_STRATEGY` (rrf, weighted, llm)
  - [ ] Add `MULTIMODAL_RERANKING_PROVIDER` (optional override)
  - [ ] Update `.env.example` with multi-modal reranking config

- [ ] **Integration with Multi-Modal Query** (AC: 6, 9)
  - [ ] Integrate with `perform_rag_query` from Story 2.5
  - [ ] Call multi-modal reranking after initial ranking
  - [ ] Pass all modality results to reranker
  - [ ] Update result ranks after reranking
  - [ ] Maintain backward compatibility with text-only

- [ ] **Performance Optimization** (AC: 11)
  - [ ] Implement batch reranking for multiple results
  - [ ] Add caching for cross-modal scores
  - [ ] Optimize LLM prompt sizes
  - [ ] Implement parallel scoring where possible
  - [ ] Add timeout handling for slow reranking

- [ ] **Quality Metrics and Evaluation** (AC: 4)
  - [ ] Implement quality metrics (NDCG, MRR, etc.)
  - [ ] Compare multi-modal vs. simple concatenation
  - [ ] Log reranking quality metrics
  - [ ] Add A/B testing support
  - [ ] Create evaluation dataset for multi-modal queries

- [ ] **Testing Implementation** (AC: 12)
  - [ ] Create `tests/test_multimodal_reranking.py`
  - [ ] Unit tests for cross-modal scoring
  - [ ] Unit tests for ranking weight application
  - [ ] Unit tests for fusion strategies
  - [ ] Test each reranking provider
  - [ ] Test performance (< 500ms overhead)
  - [ ] Test backward compatibility
  - [ ] Integration tests with full query pipeline

- [ ] **Documentation Updates** (AC: 13)
  - [ ] Update `CLAUDE.md` with multi-modal reranking configuration
  - [ ] Document cross-modal scoring algorithms
  - [ ] Document fusion strategies and trade-offs
  - [ ] Add reranking provider comparison
  - [ ] Create troubleshooting guide for reranking issues

## Dev Notes

### Existing System Integration

**Integration Points:**
- **Reranking System**: `src/ai_providers/base.py` - Reranking provider interfaces (Story 1.1)
- **Reranking Providers**: `src/ai_providers/providers/` - Existing providers with reranking
- **Multi-Modal Query**: `src/multimodal/query.py` - Integration point from Story 2.5
- **MCP Server**: `src/crawl4ai_mcp.py` - `perform_rag_query` enhancement

**Technology Stack:**
- Python 3.13 with async/await
- Existing AI provider system (Ollama, OpenAI, vLLM, HuggingFace)
- Multi-modal models (CLIP, LLaVA, GPT-4V)
- Existing reranking infrastructure from Story 1.1

### Architecture Context

**Existing Reranking (Story 1.1 - Text-Only):**
```
Text Results →
1. Send to reranking provider →
2. Provider scores each result →
3. Re-sort by reranking scores →
4. Return reranked results
```

**Multi-Modal Reranking Architecture:**
```
Mixed Results (text + images + code) →
1. Separate by modality →
2. Cross-Modal Scoring:
   ├─ Text-to-query relevance (existing)
   ├─ Image-to-query relevance (NEW)
   ├─ Code-to-query relevance (NEW)
   ├─ Text-to-image coherence (NEW)
   └─ Code-to-text coherence (NEW)
3. Apply Modality Weights:
   ├─ Text weight: 0.5
   ├─ Image weight: 0.3
   └─ Code weight: 0.2
4. Fusion Strategy:
   ├─ Reciprocal Rank Fusion (RRF)
   ├─ Weighted Score Fusion
   └─ LLM-Based Fusion
5. Final Ranking →
6. Return reranked multi-modal results
```

**Multi-Modal Reranking Provider Interface:**
```python
class MultiModalRerankingProvider(ABC):
    """Interface for multi-modal reranking"""

    @abstractmethod
    async def rerank_multimodal(
        self,
        query: str,
        results: List[MultiModalResult],
        weights: Optional[ModalityWeights] = None
    ) -> List[MultiModalResult]:
        """
        Rerank multi-modal results

        Args:
            query: Original search query
            results: Mixed results (text, images, code)
            weights: Modality-specific weights

        Returns:
            Reranked results with updated scores
        """
        pass

    @abstractmethod
    async def score_cross_modal_relevance(
        self,
        query: str,
        text_result: dict,
        image_result: dict
    ) -> float:
        """
        Score relevance between text and image

        Returns: Coherence score 0-1
        """
        pass

    def supports_multimodal_reranking(self) -> bool:
        """Check if provider supports multi-modal reranking"""
        return True
```

**Modality Weights Configuration:**
```python
@dataclass
class ModalityWeights:
    """Weights for different modalities"""
    text: float = 0.5
    image: float = 0.3
    code: float = 0.2

    def __post_init__(self):
        # Validate weights sum to 1.0
        total = self.text + self.image + self.code
        if not (0.99 <= total <= 1.01):  # Allow small floating point error
            raise ValueError(f"Weights must sum to 1.0, got {total}")

    def normalize(self):
        """Normalize weights to sum to 1.0"""
        total = self.text + self.image + self.code
        self.text /= total
        self.image /= total
        self.code /= total
```

### Key Technical Decisions

**1. Cross-Modal Scoring Approaches:**

**A. LLM-Based Scoring (High Quality, Slower):**
- Use vision-capable LLMs (GPT-4V, LLaVA, Qwen-VL)
- Prompt LLM to score text-image relevance
- Prompt example: "On a scale of 0-10, how relevant is this image to the query: '{query}'? Image description: {description}"
- Best for: OpenAI (GPT-4V), vLLM (LLaVA), Ollama (llava models)

**B. Embedding Similarity (Faster, Lower Quality):**
- Use CLIP or similar multi-modal embeddings
- Calculate cosine similarity between text and image embeddings
- Best for: HuggingFace provider, fallback for others

**C. Hybrid Approach:**
- Use LLM for top-k results
- Use embeddings for remaining results
- Balance quality and performance

**2. Fusion Strategies:**

**A. Reciprocal Rank Fusion (RRF):**
```python
def reciprocal_rank_fusion(
    modality_results: Dict[str, List[Result]],
    k: int = 60
) -> List[Result]:
    """
    RRF fusion across modalities

    Score = sum(1 / (k + rank)) for each modality
    """
    fused_scores = {}

    for modality, results in modality_results.items():
        for rank, result in enumerate(results, start=1):
            result_id = result["id"]
            if result_id not in fused_scores:
                fused_scores[result_id] = {"result": result, "score": 0.0}
            fused_scores[result_id]["score"] += 1 / (k + rank)

    # Sort by fused score
    return sorted(
        fused_scores.values(),
        key=lambda x: x["score"],
        reverse=True
    )
```

**B. Weighted Score Fusion:**
```python
def weighted_score_fusion(
    results: List[MultiModalResult],
    weights: ModalityWeights
) -> List[MultiModalResult]:
    """
    Apply modality weights to scores

    weighted_score = modality_weight * normalized_score
    """
    for result in results:
        modality = result["modality"]
        base_score = result["normalized_score"]

        if modality == "text":
            result["weighted_score"] = weights.text * base_score
        elif modality == "image":
            result["weighted_score"] = weights.image * base_score
        elif modality == "code":
            result["weighted_score"] = weights.code * base_score

    return sorted(results, key=lambda x: x["weighted_score"], reverse=True)
```

**C. LLM-Based Fusion (Most Expensive):**
- Send all results to LLM
- Ask LLM to rank results holistically
- Consider cross-modal coherence
- Best quality but highest latency/cost

**3. Provider-Specific Implementations:**

**vLLM Multi-Modal Reranking (Preferred):**
- Use vision models for cross-modal scoring
- Fast inference via cloud-deployed vLLM
- Batch scoring for performance

**Ollama Multi-Modal Reranking:**
- Use llava models if available
- Fallback to embedding similarity
- Local processing (privacy)

**OpenAI Multi-Modal Reranking:**
- Use GPT-4V if available
- High quality but expensive
- Implement caching to reduce costs

**HuggingFace Multi-Modal Reranking:**
- Use CLIP embeddings
- Fast local inference
- Good balance of quality and performance

**4. Performance Optimization:**
- Batch reranking requests
- Cache cross-modal scores
- Parallel scoring where possible
- Timeout handling (500ms target)
- Skip reranking for very low-scored results

**5. Backward Compatibility:**
- Text-only reranking unchanged
- Multi-modal reranking opt-in via USE_MULTIMODAL_RAG
- Fallback to simple concatenation if reranking fails

### Configuration Examples

```bash
# Enable multi-modal reranking (requires USE_MULTIMODAL_RAG=true)
USE_RERANKING=true
USE_MULTIMODAL_RAG=true

# Reranking provider (optional override, defaults to AI_PROVIDER)
RERANKING_PROVIDER=vllm  # or ollama, openai, huggingface

# Modality weights (must sum to 1.0)
MULTIMODAL_RANKING_TEXT_WEIGHT=0.5
MULTIMODAL_RANKING_IMAGE_WEIGHT=0.3
MULTIMODAL_RANKING_CODE_WEIGHT=0.2

# Fusion strategy
MULTIMODAL_FUSION_STRATEGY=rrf  # or weighted, llm, hybrid

# Cross-modal scoring method
MULTIMODAL_SCORING_METHOD=llm  # or embedding, hybrid
MULTIMODAL_LLM_SCORING_TOP_K=10  # Use LLM for top-10, embeddings for rest

# Performance tuning
MULTIMODAL_RERANKING_TIMEOUT_MS=500
MULTIMODAL_RERANKING_BATCH_SIZE=20
MULTIMODAL_CACHE_CROSS_MODAL_SCORES=true

# Provider-specific configuration
# vLLM
VLLM_VISION_MODEL=llava-hf/llava-v1.6-mistral-7b-hf

# Ollama (use llava for multi-modal reranking)
OLLAMA_VISION_MODEL=llava:7b

# OpenAI (use GPT-4V for multi-modal reranking)
OPENAI_VISION_MODEL=gpt-4-vision-preview

# HuggingFace (use CLIP for multi-modal embeddings)
HUGGINGFACE_MULTIMODAL_MODEL=openai/clip-vit-base-patch32
```

### Implementation Guidelines

**vLLM Multi-Modal Reranking Implementation:**
```python
class VLLMProvider(HybridAIProvider, VisionProvider, MultiModalRerankingProvider):
    """vLLM provider with multi-modal reranking"""

    async def rerank_multimodal(
        self,
        query: str,
        results: List[MultiModalResult],
        weights: Optional[ModalityWeights] = None
    ) -> List[MultiModalResult]:
        """
        Rerank multi-modal results using vLLM vision model
        """
        weights = weights or self._get_default_weights()

        # Score each result
        scored_results = []
        for result in results:
            # Get base relevance score
            if result["modality"] == "text":
                score = await self._score_text_relevance(query, result)
            elif result["modality"] == "image":
                score = await self._score_image_relevance(query, result)
            elif result["modality"] == "code":
                score = await self._score_code_relevance(query, result)

            # Apply modality weight
            weighted_score = score * getattr(weights, result["modality"])
            result["reranking_score"] = weighted_score
            scored_results.append(result)

        # Sort by reranking score
        return sorted(scored_results, key=lambda x: x["reranking_score"], reverse=True)

    async def _score_image_relevance(
        self,
        query: str,
        image_result: dict
    ) -> float:
        """
        Score image relevance to query using vision model
        """
        # Construct prompt
        prompt = f"""
Given the search query: "{query}"

And this image with description: "{image_result['description']}"

Rate the relevance of this image to the query on a scale of 0.0 to 1.0.
Consider both the image description and any extracted text.

Reply with only the numeric score.
        """.strip()

        # Call vision model
        try:
            response = await self._call_vision_endpoint(
                endpoint="/v1/chat/completions",
                image=image_result.get("storage_path"),
                model=self.config.vision_model,
                messages=[{"role": "user", "content": prompt}]
            )

            # Parse score from response
            score_text = response["choices"][0]["message"]["content"]
            score = float(score_text)
            return max(0.0, min(1.0, score))  # Clamp to 0-1

        except Exception as e:
            logger.warning(f"Vision-based scoring failed, using embedding: {e}")
            # Fallback to embedding similarity
            return await self._score_via_embedding_similarity(query, image_result)

    async def score_cross_modal_relevance(
        self,
        query: str,
        text_result: dict,
        image_result: dict
    ) -> float:
        """
        Score coherence between text and image results
        """
        prompt = f"""
Given a search query: "{query}"

Text content: "{text_result['content'][:500]}..."
Image description: "{image_result['description']}"

How coherent and complementary are the text and image?
Do they discuss the same topic? Rate 0.0-1.0.

Reply with only the numeric score.
        """.strip()

        try:
            response = await self._call_llm(prompt)
            score = float(response)
            return max(0.0, min(1.0, score))
        except:
            # Fallback: check if from same source page
            same_source = text_result.get("source_page_id") == image_result.get("source_page_id")
            return 0.8 if same_source else 0.3
```

**Reciprocal Rank Fusion Implementation:**
```python
async def reciprocal_rank_fusion_multimodal(
    query: str,
    results: List[MultiModalResult],
    k: int = 60
) -> List[MultiModalResult]:
    """
    Apply RRF fusion across modalities

    Benefits:
    - No score normalization needed
    - Robust to different score scales
    - Considers position in each modality's ranking
    """
    # Group by modality
    by_modality = {
        "text": [],
        "image": [],
        "code": []
    }
    for result in results:
        modality = result["modality"]
        if modality in by_modality:
            by_modality[modality].append(result)

    # Sort each modality by its original score
    for modality in by_modality:
        by_modality[modality].sort(
            key=lambda x: x.get("normalized_score", 0),
            reverse=True
        )

    # Calculate RRF scores
    rrf_scores = {}
    for modality, modality_results in by_modality.items():
        for rank, result in enumerate(modality_results, start=1):
            result_id = result.get("id") or result.get("image_url") or result.get("code")
            if result_id not in rrf_scores:
                rrf_scores[result_id] = {
                    "result": result,
                    "rrf_score": 0.0
                }
            rrf_scores[result_id]["rrf_score"] += 1 / (k + rank)

    # Sort by RRF score
    ranked = sorted(
        rrf_scores.values(),
        key=lambda x: x["rrf_score"],
        reverse=True
    )

    # Update results with RRF scores and ranks
    final_results = []
    for rank, item in enumerate(ranked, start=1):
        result = item["result"]
        result["reranking_score"] = item["rrf_score"]
        result["reranked_position"] = rank
        final_results.append(result)

    return final_results
```

**Integration with perform_rag_query:**
```python
@mcp_server.tool()
async def perform_rag_query(
    query: str,
    source_filter: Optional[str] = None,
    limit: int = 10,
    include_images: bool = True,
    include_code: bool = True,
    image_limit: int = 5
) -> str:
    """
    Perform multi-modal RAG query with reranking
    """
    # ... (initial search from Story 2.5)

    # Merge results
    merged_results = merge_multimodal_results(...)

    # Apply multi-modal reranking (NEW - Story 2.6)
    if USE_RERANKING and USE_MULTIMODAL_RAG and multimodal_enabled:
        reranking_start = perf_counter()

        # Get reranking provider
        reranking_provider = await get_reranking_provider()

        # Check if provider supports multi-modal reranking
        if isinstance(reranking_provider, MultiModalRerankingProvider):
            # Get modality weights
            weights = ModalityWeights(
                text=float(os.getenv("MULTIMODAL_RANKING_TEXT_WEIGHT", "0.5")),
                image=float(os.getenv("MULTIMODAL_RANKING_IMAGE_WEIGHT", "0.3")),
                code=float(os.getenv("MULTIMODAL_RANKING_CODE_WEIGHT", "0.2"))
            )

            # Rerank multi-modal results
            merged_results = await reranking_provider.rerank_multimodal(
                query=query,
                results=merged_results,
                weights=weights
            )

            reranking_time_ms = (perf_counter() - reranking_start) * 1000
            logger.info(f"Multi-modal reranking completed in {reranking_time_ms}ms")
        else:
            # Fallback: separate reranking per modality
            merged_results = await fallback_rerank_by_modality(
                query, merged_results, reranking_provider
            )

    # Format and return response
    # ...
```

### Testing

#### Testing Standards
- **Test file location:** `tests/test_multimodal_reranking.py`
- **Test frameworks:** pytest with asyncio support
- **Test data:** Mock multi-modal results
- **Coverage target:** 85%+ for reranking code

#### Test Scenarios

1. **Cross-Modal Scoring:**
   - Score text-to-query relevance
   - Score image-to-query relevance
   - Score code-to-query relevance
   - Score text-to-image coherence
   - Test LLM-based scoring
   - Test embedding-based scoring

2. **Modality Weights:**
   - Apply custom weights
   - Validate weight normalization
   - Test weight configuration
   - Test default weights

3. **Fusion Strategies:**
   - Test RRF fusion
   - Test weighted score fusion
   - Test LLM-based fusion
   - Compare fusion strategies
   - Test quality improvement

4. **Provider Implementations:**
   - Test vLLM multi-modal reranking
   - Test Ollama multi-modal reranking
   - Test OpenAI multi-modal reranking
   - Test HuggingFace multi-modal reranking
   - Test fallback mechanisms

5. **Performance:**
   - Measure reranking latency
   - Verify < 500ms overhead
   - Test batch processing
   - Test caching effectiveness
   - Test timeout handling

6. **Integration:**
   - Test with full query pipeline
   - Test with different modality mixes
   - Test backward compatibility (text-only)
   - Test graceful degradation

7. **Quality Metrics:**
   - Calculate NDCG scores
   - Compare with baseline (no reranking)
   - Test on evaluation dataset
   - Measure improvement percentage

8. **Configuration:**
   - Test weight configuration
   - Test fusion strategy selection
   - Test provider selection
   - Test fallback behavior

#### Example Test Structure

```python
import pytest
from src.ai_providers.providers.vllm_provider import VLLMProvider
from src.multimodal.reranking import reciprocal_rank_fusion_multimodal

@pytest.mark.asyncio
async def test_cross_modal_scoring():
    """Test scoring relevance between text and images"""
    provider = VLLMProvider(config)

    text_result = {"content": "authentication flow", "modality": "text"}
    image_result = {
        "description": "login screen diagram",
        "modality": "image"
    }

    score = await provider.score_cross_modal_relevance(
        query="user authentication",
        text_result=text_result,
        image_result=image_result
    )

    assert 0.0 <= score <= 1.0
    assert score > 0.5  # Should be relevant

@pytest.mark.asyncio
async def test_rerank_multimodal():
    """Test multi-modal reranking"""
    provider = VLLMProvider(config)
    results = [
        {"modality": "text", "content": "...", "normalized_score": 0.8},
        {"modality": "image", "description": "...", "normalized_score": 0.75},
        {"modality": "code", "code": "...", "normalized_score": 0.7}
    ]

    weights = ModalityWeights(text=0.5, image=0.3, code=0.2)

    reranked = await provider.rerank_multimodal(
        query="test query",
        results=results,
        weights=weights
    )

    # Verify reranking scores applied
    assert all("reranking_score" in r for r in reranked)
    # Verify sorted by reranking score
    scores = [r["reranking_score"] for r in reranked]
    assert scores == sorted(scores, reverse=True)

def test_reciprocal_rank_fusion():
    """Test RRF fusion across modalities"""
    results = [
        {"id": "1", "modality": "text", "normalized_score": 0.9},
        {"id": "2", "modality": "image", "normalized_score": 0.85},
        {"id": "3", "modality": "text", "normalized_score": 0.8},
        {"id": "4", "modality": "code", "normalized_score": 0.75}
    ]

    fused = reciprocal_rank_fusion_multimodal("query", results, k=60)

    # Verify RRF scores calculated
    assert all("rrf_score" in r for r in fused)
    # Verify reranked
    assert fused != results

def test_modality_weights():
    """Test modality weight application"""
    weights = ModalityWeights(text=0.5, image=0.3, code=0.2)
    assert abs(weights.text + weights.image + weights.code - 1.0) < 0.01

    # Test invalid weights
    with pytest.raises(ValueError):
        ModalityWeights(text=0.5, image=0.5, code=0.5)  # Sum > 1.0

@pytest.mark.asyncio
async def test_multimodal_reranking_performance():
    """Test reranking meets performance requirements"""
    from time import perf_counter

    provider = VLLMProvider(config)
    results = [create_mock_result() for _ in range(20)]

    start = perf_counter()
    reranked = await provider.rerank_multimodal("query", results)
    elapsed_ms = (perf_counter() - start) * 1000

    # Verify < 500ms overhead
    assert elapsed_ms < 500, f"Reranking took {elapsed_ms}ms (expected < 500ms)"

@pytest.mark.asyncio
async def test_multimodal_reranking_quality():
    """Test reranking improves result quality"""
    # Create test dataset with known relevance
    test_cases = load_evaluation_dataset()

    for test_case in test_cases:
        query = test_case["query"]
        results = test_case["results"]
        expected_top_result = test_case["expected_top_result"]

        # Rerank
        reranked = await provider.rerank_multimodal(query, results)

        # Check if expected result is now top-ranked
        assert reranked[0]["id"] == expected_top_result["id"]

    # Calculate overall improvement
    ndcg_before = calculate_ndcg(results)
    ndcg_after = calculate_ndcg(reranked)
    improvement = (ndcg_after - ndcg_before) / ndcg_before

    # Verify measurable improvement
    assert improvement > 0.1, f"Improvement {improvement:.2%} < 10%"
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-01 | 1.0 | Initial story creation for multi-modal reranking | Sarah (PO) |

## Dev Agent Record

### Agent Model Used
_(To be populated by dev agent during implementation)_

### Debug Log References
_(To be populated by dev agent during implementation)_

### Completion Notes List
_(To be populated by dev agent during implementation)_

### File List
**Expected Files to be Created/Modified:**
- `src/ai_providers/base.py` (modified - add MultiModalRerankingProvider)
- `src/ai_providers/providers/vllm_provider.py` (modified - implement multi-modal reranking)
- `src/ai_providers/providers/ollama_provider.py` (modified - implement multi-modal reranking)
- `src/ai_providers/providers/openai_provider.py` (modified - implement multi-modal reranking)
- `src/ai_providers/providers/huggingface_provider.py` (modified - implement multi-modal reranking)
- `src/multimodal/reranking.py` (new - fusion strategies)
- `src/multimodal/scoring.py` (new - cross-modal scoring)
- `src/multimodal/evaluation.py` (new - quality metrics)
- `src/crawl4ai_mcp.py` (modified - integrate multi-modal reranking)
- `tests/test_multimodal_reranking.py` (new)
- `tests/fixtures/evaluation_dataset.json` (new - test data)
- `.env.example` (modified - add multi-modal reranking config)
- `CLAUDE.md` (modified - add multi-modal reranking docs)

## QA Results
_(To be populated by QA agent after implementation)_
