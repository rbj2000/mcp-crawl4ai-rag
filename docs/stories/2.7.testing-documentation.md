# Story 2.7: Testing, Documentation, and Validation

## Status
Draft

## Story
**As a** developer and user of the system,
**I want** comprehensive testing and documentation for multi-modal features,
**so that** I can confidently deploy and use the enhanced system.

## Acceptance Criteria

### Functional Requirements
1. Test coverage > 80% for new multi-modal code
2. Integration tests validate full multi-modal workflow (crawl → store → search → retrieve)
3. Performance benchmarks documented (baseline vs. multi-modal)
4. CLAUDE.md updated with vLLM and multi-modal configuration
5. Migration guide for database schema updates
6. Example documentation site with images crawled and searchable

### Integration Requirements
7. End-to-end tests covering all stories (2.1-2.6)
8. Tests for all database providers (Supabase, SQLite, Neo4j, Pinecone, Weaviate)
9. Tests for all AI providers (OpenAI, Ollama, vLLM, HuggingFace)
10. Backward compatibility tests (text-only mode unchanged)

### Quality Requirements
11. Automated CI/CD integration for multi-modal tests
12. Performance regression tests (no slowdown in text-only mode)
13. Example queries and expected results documented
14. Troubleshooting guide for common issues

## Tasks / Subtasks

- [ ] **Unit Test Coverage** (AC: 1)
  - [ ] Calculate current test coverage for multi-modal code
  - [ ] Identify gaps in unit test coverage
  - [ ] Add missing unit tests to reach 80% coverage
  - [ ] Test all multi-modal components (vision, code, query, reranking)
  - [ ] Add code coverage reporting to CI/CD
  - [ ] Document coverage targets and requirements

- [ ] **Integration Test Suite** (AC: 2, 7)
  - [ ] Create `tests/integration/test_multimodal_workflow.py`
  - [ ] Test end-to-end crawl with images
  - [ ] Test image storage and retrieval
  - [ ] Test multi-modal query with text + images
  - [ ] Test vision model processing
  - [ ] Test multi-modal reranking
  - [ ] Test with different provider combinations

- [ ] **Database Provider Testing** (AC: 8)
  - [ ] Test Supabase with multi-modal data
  - [ ] Test SQLite with multi-modal data
  - [ ] Test Neo4j with multi-modal data
  - [ ] Test Pinecone with multi-modal data
  - [ ] Test Weaviate with multi-modal data
  - [ ] Create database-specific test fixtures
  - [ ] Test schema migrations for each provider

- [ ] **AI Provider Testing** (AC: 9)
  - [ ] Test vLLM provider (text + vision)
  - [ ] Test OpenAI provider with multi-modal
  - [ ] Test Ollama provider with multi-modal
  - [ ] Test HuggingFace provider with multi-modal
  - [ ] Test provider fallback mechanisms
  - [ ] Test mixed provider configurations

- [ ] **Performance Benchmarking** (AC: 3, 12)
  - [ ] Create performance benchmarking suite
  - [ ] Benchmark text-only crawling (baseline)
  - [ ] Benchmark multi-modal crawling (with images)
  - [ ] Benchmark text-only queries (baseline)
  - [ ] Benchmark multi-modal queries (with images)
  - [ ] Measure vision model processing time
  - [ ] Document performance metrics and comparison
  - [ ] Create performance regression tests

- [ ] **Backward Compatibility Testing** (AC: 10, 12)
  - [ ] Test text-only mode with USE_MULTIMODAL_RAG=false
  - [ ] Verify no breaking changes to existing APIs
  - [ ] Test existing MCP tools work unchanged
  - [ ] Test database schema backward compatibility
  - [ ] Test configuration backward compatibility
  - [ ] Create backward compatibility test suite

- [ ] **CLAUDE.md Documentation Updates** (AC: 4)
  - [ ] Add vLLM provider configuration section
  - [ ] Add multi-modal RAG configuration section
  - [ ] Document image storage configuration
  - [ ] Document vision model configuration
  - [ ] Document enhanced code processing
  - [ ] Document multi-modal reranking
  - [ ] Add Docker Compose profiles for multi-modal
  - [ ] Update quick start examples

- [ ] **Migration Guide** (AC: 5)
  - [ ] Create `docs/migration/multimodal_migration_guide.md`
  - [ ] Document database schema changes
  - [ ] Provide migration SQL scripts
  - [ ] Document configuration changes
  - [ ] Document breaking changes (if any)
  - [ ] Provide rollback procedures
  - [ ] Document upgrade path from text-only to multi-modal

- [ ] **Example Documentation Site** (AC: 6)
  - [ ] Select example documentation site with images
  - [ ] Crawl example site with multi-modal enabled
  - [ ] Verify images extracted and stored
  - [ ] Verify vision model processing
  - [ ] Create example queries for different modalities
  - [ ] Document expected results
  - [ ] Create demo scripts/notebooks

- [ ] **Example Queries and Results** (AC: 13)
  - [ ] Document 10+ example multi-modal queries
  - [ ] Document expected result structure
  - [ ] Include text-only queries
  - [ ] Include image-focused queries
  - [ ] Include code-focused queries
  - [ ] Include mixed-modality queries
  - [ ] Add query performance benchmarks

- [ ] **Troubleshooting Guide** (AC: 14)
  - [ ] Create `docs/troubleshooting/multimodal_troubleshooting.md`
  - [ ] Document common issues and solutions
  - [ ] Add debugging procedures
  - [ ] Document error messages and meanings
  - [ ] Add provider-specific troubleshooting
  - [ ] Add performance troubleshooting
  - [ ] Include FAQ section

- [ ] **CI/CD Integration** (AC: 11)
  - [ ] Add multi-modal tests to CI/CD pipeline
  - [ ] Configure test matrix (providers, databases)
  - [ ] Add performance regression checks
  - [ ] Add code coverage reporting
  - [ ] Configure test artifacts and reports
  - [ ] Add integration test environment setup

- [ ] **API Documentation** (AC: 4)
  - [ ] Document multi-modal MCP tool signatures
  - [ ] Document result structure and schemas
  - [ ] Add API examples with multi-modal
  - [ ] Document configuration options
  - [ ] Create OpenAPI/JSON schema if applicable
  - [ ] Add code examples in Python

- [ ] **Architecture Documentation** (AC: 4)
  - [ ] Create architecture diagrams for multi-modal system
  - [ ] Document data flow (crawl → store → query)
  - [ ] Document provider integration architecture
  - [ ] Document vision model integration
  - [ ] Document multi-modal query flow
  - [ ] Add sequence diagrams for key operations

- [ ] **Quality Assurance** (AC: 1-14)
  - [ ] Run full test suite and verify pass rate
  - [ ] Perform manual QA testing
  - [ ] Test on different operating systems
  - [ ] Test with different Python versions (3.11, 3.12, 3.13)
  - [ ] Load testing for multi-modal queries
  - [ ] Security review of image storage

## Dev Notes

### Existing System Integration

**Integration Points:**
- **Test Framework**: `tests/` - Existing pytest infrastructure
- **Configuration**: `.env.example` - Environment variable documentation
- **Documentation**: `CLAUDE.md` - Main project documentation
- **CI/CD**: GitHub Actions or equivalent pipeline
- **Coverage**: pytest-cov for code coverage reporting

**Technology Stack:**
- pytest for unit and integration tests
- pytest-asyncio for async tests
- pytest-cov for coverage reporting
- Mock libraries for provider testing
- Performance profiling tools (cProfile, time)

### Architecture Context

**Testing Strategy:**
```
1. Unit Tests (80%+ coverage):
   ├─ vLLM provider tests
   ├─ Image storage tests
   ├─ Vision model integration tests
   ├─ Code processing tests
   ├─ Multi-modal query tests
   └─ Reranking tests

2. Integration Tests:
   ├─ End-to-end crawl → search workflow
   ├─ Database provider integration
   ├─ AI provider integration
   └─ MCP tool integration

3. Performance Tests:
   ├─ Crawl performance benchmarks
   ├─ Query latency benchmarks
   ├─ Vision model processing benchmarks
   └─ Regression tests vs. baseline

4. Backward Compatibility Tests:
   ├─ Text-only mode validation
   ├─ API compatibility checks
   └─ Configuration compatibility checks
```

**Test Structure:**
```
tests/
├── unit/
│   ├── test_vllm_provider.py (Story 2.1)
│   ├── test_image_extraction.py (Story 2.2)
│   ├── test_vision_model_integration.py (Story 2.3)
│   ├── test_enhanced_code_processing.py (Story 2.4)
│   ├── test_multimodal_rag_query.py (Story 2.5)
│   └── test_multimodal_reranking.py (Story 2.6)
├── integration/
│   ├── test_multimodal_workflow.py
│   ├── test_database_providers.py
│   └── test_ai_providers.py
├── performance/
│   ├── benchmark_crawling.py
│   ├── benchmark_queries.py
│   └── test_performance_regression.py
├── backward_compatibility/
│   └── test_text_only_mode.py
└── fixtures/
    ├── test_images/
    ├── test_documents/
    └── test_data/
```

### Key Technical Decisions

**1. Test Coverage Strategy:**
- Focus on new multi-modal code (Stories 2.1-2.6)
- Maintain existing test coverage for unchanged code
- Use branch coverage, not just line coverage
- Prioritize critical paths and error handling

**2. Integration Test Approach:**
- Use real database instances (not mocks) when possible
- Use Docker Compose for test environment setup
- Mock external APIs (vLLM, OpenAI) to avoid costs
- Create reusable test fixtures

**3. Performance Benchmarking:**
- Establish baseline metrics before multi-modal
- Measure p50, p95, p99 latencies
- Compare text-only vs. multi-modal performance
- Set regression thresholds (e.g., < 5% slowdown)

**4. Documentation Structure:**
- Keep CLAUDE.md as single source of truth
- Separate guides for complex topics (migration, troubleshooting)
- Include code examples in documentation
- Provide both quick start and detailed configuration

**5. Example Documentation Site:**
- Use a real documentation site (e.g., FastAPI docs, Django docs)
- Include variety of content types (text, diagrams, screenshots, code)
- Demonstrate all multi-modal features
- Provide reproducible demo scripts

### Configuration Examples

```bash
# Test Configuration
PYTEST_COVERAGE_TARGET=80
PYTEST_TIMEOUT=300  # 5 minutes for integration tests
PYTEST_ASYNCIO_MODE=auto

# Performance Benchmarking
BENCHMARK_ITERATIONS=10
BENCHMARK_WARMUP_ITERATIONS=3
PERFORMANCE_REGRESSION_THRESHOLD=1.05  # 5% slowdown allowed

# Integration Test Environment
TEST_VLLM_ENDPOINT=http://localhost:8000
TEST_OLLAMA_ENDPOINT=http://localhost:11434
TEST_DATABASE_PROVIDER=sqlite  # Use SQLite for fast tests
TEST_IMAGE_STORAGE_PATH=/tmp/test_images

# CI/CD Configuration
CI_RUN_INTEGRATION_TESTS=true
CI_RUN_PERFORMANCE_TESTS=false  # Too slow for every commit
CI_COVERAGE_FAIL_UNDER=80
```

### Implementation Guidelines

**End-to-End Integration Test:**
```python
import pytest
from src.crawl4ai_mcp import crawl_single_page, perform_rag_query
import os

@pytest.mark.asyncio
@pytest.mark.integration
async def test_multimodal_workflow_end_to_end():
    """
    Test complete multi-modal workflow:
    1. Crawl page with images
    2. Extract and store images
    3. Process images with vision model
    4. Query with multi-modal search
    5. Verify results include images
    """
    # Setup
    os.environ["USE_MULTIMODAL_RAG"] = "true"
    os.environ["EXTRACT_IMAGES"] = "true"
    os.environ["GENERATE_IMAGE_CAPTIONS"] = "true"

    test_url = "https://example.com/docs/architecture"  # Page with diagrams

    # Step 1: Crawl page with images
    crawl_result = await crawl_single_page(test_url)
    assert "success" in crawl_result.lower()

    # Verify images were extracted
    from src.database import get_database_provider
    db = await get_database_provider()
    images = await db.get_images_for_page(test_url)
    assert len(images) > 0, "No images extracted from page"

    # Verify vision model processing
    for image in images:
        assert image.get("description"), "Image missing description"
        assert image.get("embedding"), "Image missing embedding"

    # Step 2: Multi-modal query
    query = "system architecture diagram"
    result_json = await perform_rag_query(
        query=query,
        include_images=True
    )

    result = json.loads(result_json)
    results = result["results"]

    # Verify multi-modal results
    modalities = {r["modality"] for r in results}
    assert "text" in modalities, "Missing text results"
    assert "image" in modalities, "Missing image results"

    # Verify image results have required fields
    image_results = [r for r in results if r["modality"] == "image"]
    assert len(image_results) > 0, "No image results returned"

    for img_result in image_results:
        assert "image_url" in img_result or "storage_path" in img_result
        assert "description" in img_result
        assert "similarity_score" in img_result

    # Verify performance
    metrics = result["performance_metrics"]
    assert metrics["total_time_ms"] < 2000, "Query too slow"

    # Verify results are relevant
    # At least one result should be highly relevant
    top_result = results[0]
    assert top_result["similarity_score"] > 0.7, "Top result not relevant enough"

    print(f"✓ Multi-modal workflow test passed")
    print(f"  - Extracted {len(images)} images")
    print(f"  - Query returned {len(results)} results")
    print(f"  - Top result: {top_result['modality']} with score {top_result['similarity_score']}")
```

**Performance Benchmark Suite:**
```python
import pytest
from time import perf_counter
import statistics

@pytest.mark.benchmark
class TestMultiModalPerformance:
    """Performance benchmarks for multi-modal features"""

    @pytest.mark.asyncio
    async def test_crawl_performance_baseline_text_only(self):
        """Benchmark: Text-only crawling (baseline)"""
        os.environ["USE_MULTIMODAL_RAG"] = "false"
        os.environ["EXTRACT_IMAGES"] = "false"

        timings = []
        for i in range(10):
            start = perf_counter()
            await crawl_single_page(f"https://example.com/page{i}")
            elapsed = perf_counter() - start
            timings.append(elapsed)

        avg_time = statistics.mean(timings)
        p95_time = statistics.quantiles(timings, n=20)[18]  # 95th percentile

        print(f"Text-only crawl: avg={avg_time:.2f}s, p95={p95_time:.2f}s")
        return {"avg": avg_time, "p95": p95_time}

    @pytest.mark.asyncio
    async def test_crawl_performance_multimodal(self):
        """Benchmark: Multi-modal crawling with images"""
        os.environ["USE_MULTIMODAL_RAG"] = "true"
        os.environ["EXTRACT_IMAGES"] = "true"
        os.environ["GENERATE_IMAGE_CAPTIONS"] = "true"

        timings = []
        for i in range(10):
            start = perf_counter()
            await crawl_single_page(f"https://example.com/page{i}")
            elapsed = perf_counter() - start
            timings.append(elapsed)

        avg_time = statistics.mean(timings)
        p95_time = statistics.quantiles(timings, n=20)[18]

        print(f"Multi-modal crawl: avg={avg_time:.2f}s, p95={p95_time:.2f}s")

        # Compare with baseline
        baseline = await self.test_crawl_performance_baseline_text_only()
        overhead = (avg_time - baseline["avg"]) / baseline["avg"]

        # Verify < 30% overhead requirement (Story 2.2)
        assert overhead < 0.30, f"Overhead {overhead:.1%} exceeds 30% limit"

        return {"avg": avg_time, "p95": p95_time, "overhead": overhead}

    @pytest.mark.asyncio
    async def test_query_performance_multimodal(self):
        """Benchmark: Multi-modal query latency"""
        os.environ["USE_MULTIMODAL_RAG"] = "true"

        queries = [
            "authentication flow",
            "architecture diagram",
            "code example for API",
            "database schema",
            "deployment guide"
        ]

        timings = []
        for query in queries:
            start = perf_counter()
            await perform_rag_query(query, include_images=True)
            elapsed = (perf_counter() - start) * 1000  # ms
            timings.append(elapsed)

        avg_time = statistics.mean(timings)
        p95_time = statistics.quantiles(timings, n=20)[18]

        print(f"Multi-modal query: avg={avg_time:.0f}ms, p95={p95_time:.0f}ms")

        # Verify < 2s p95 latency requirement (Story 2.5)
        assert p95_time < 2000, f"p95 latency {p95_time:.0f}ms exceeds 2000ms"

        return {"avg": avg_time, "p95": p95_time}
```

**Backward Compatibility Test Suite:**
```python
@pytest.mark.backward_compatibility
class TestBackwardCompatibility:
    """Ensure text-only mode unchanged by multi-modal features"""

    @pytest.mark.asyncio
    async def test_text_only_mode_unchanged(self):
        """Verify text-only mode works exactly as before"""
        # Disable all multi-modal features
        os.environ["USE_MULTIMODAL_RAG"] = "false"
        os.environ["EXTRACT_IMAGES"] = "false"
        os.environ["USE_ENHANCED_CODE_PROCESSING"] = "false"

        # Test crawling
        result = await crawl_single_page("https://example.com/text-page")
        assert "success" in result.lower()

        # Test querying
        query_result = await perform_rag_query("test query")
        result_data = json.loads(query_result)

        # Verify only text results
        for r in result_data["results"]:
            assert r["modality"] == "text", "Non-text result in text-only mode"

        # Verify no image-related fields
        assert "image_url" not in str(result_data)

    @pytest.mark.asyncio
    async def test_existing_api_signatures_unchanged(self):
        """Verify existing MCP tool signatures work unchanged"""
        # Call with old signature (no new parameters)
        result = await perform_rag_query(
            query="test",
            source_filter=None,
            limit=10
            # No include_images, include_code parameters
        )

        # Should work without errors
        assert result is not None

    @pytest.mark.asyncio
    async def test_database_schema_backward_compatible(self):
        """Verify old queries work with new schema"""
        db = await get_database_provider()

        # Old query (text only)
        results = await db.query_crawled_pages("test query", limit=10)
        assert len(results) >= 0  # Should work

        # Verify old fields still present
        if results:
            result = results[0]
            assert "content" in result
            assert "url" in result
            # New fields should be optional/nullable
```

### Example Documentation Structure

**CLAUDE.md Updates:**
```markdown
## Multi-Modal RAG Configuration

### vLLM Provider

Configure vLLM for text and vision model inference:

\`\`\`bash
# vLLM Provider
AI_PROVIDER=vllm
VLLM_BASE_URL=https://your-vllm-endpoint.com/v1
VLLM_API_KEY=your_api_key
VLLM_TEXT_MODEL=meta-llama/Llama-3.1-8B-Instruct
VLLM_VISION_MODEL=llava-hf/llava-v1.6-mistral-7b-hf
VLLM_EMBEDDING_DIMENSIONS=768
\`\`\`

### Multi-Modal RAG

Enable image extraction and multi-modal search:

\`\`\`bash
# Enable multi-modal features
USE_MULTIMODAL_RAG=true
EXTRACT_IMAGES=true
GENERATE_IMAGE_CAPTIONS=true
EXTRACT_IMAGE_TEXT=true  # OCR

# Image storage
IMAGE_STORAGE_TYPE=filesystem
IMAGE_STORAGE_PATH=/app/data/images
IMAGE_MAX_SIZE_MB=10

# Result limits
MULTIMODAL_MAX_TEXT_RESULTS=10
MULTIMODAL_MAX_IMAGE_RESULTS=5
\`\`\`

### Example Queries

**Text + Image Query:**
\`\`\`python
result = await perform_rag_query(
    query="system architecture diagram",
    include_images=True,
    image_limit=5
)
\`\`\`

**Result Structure:**
\`\`\`json
{
  "query": "system architecture diagram",
  "results": [
    {
      "modality": "image",
      "image_url": "https://...",
      "description": "Architecture diagram showing microservices",
      "similarity_score": 0.89
    },
    {
      "modality": "text",
      "content": "Our system uses a microservices architecture...",
      "similarity_score": 0.85
    }
  ]
}
\`\`\`

### Docker Compose Profiles

**Multi-Modal with vLLM:**
\`\`\`bash
docker compose --profile vllm-multimodal up -d
\`\`\`

See [Multi-Modal Migration Guide](docs/migration/multimodal_migration_guide.md) for upgrading.
```

### Testing

#### Testing Standards
- **Coverage target:** 80%+ for new code
- **Integration tests:** Full end-to-end workflows
- **Performance tests:** Benchmark and regression tests
- **Backward compatibility:** Verify text-only mode unchanged

#### Test Execution
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run only unit tests
pytest tests/unit/

# Run integration tests
pytest -m integration tests/integration/

# Run performance benchmarks
pytest -m benchmark tests/performance/

# Run backward compatibility tests
pytest -m backward_compatibility
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-01 | 1.0 | Initial story creation for testing and documentation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used
_(To be populated by dev agent during implementation)_

### Debug Log References
_(To be populated by dev agent during implementation)_

### Completion Notes List
_(To be populated by dev agent during implementation)_

### File List
**Expected Files to be Created/Modified:**
- `tests/integration/test_multimodal_workflow.py` (new)
- `tests/integration/test_database_providers.py` (new)
- `tests/integration/test_ai_providers.py` (new)
- `tests/performance/benchmark_crawling.py` (new)
- `tests/performance/benchmark_queries.py` (new)
- `tests/performance/test_performance_regression.py` (new)
- `tests/backward_compatibility/test_text_only_mode.py` (new)
- `docs/migration/multimodal_migration_guide.md` (new)
- `docs/troubleshooting/multimodal_troubleshooting.md` (new)
- `docs/examples/multimodal_queries.md` (new)
- `docs/architecture/multimodal_architecture.md` (new)
- `CLAUDE.md` (modified - comprehensive updates)
- `.github/workflows/test_multimodal.yml` (new - CI/CD)
- `pytest.ini` (modified - test configuration)
- `conftest.py` (modified - test fixtures)
- `scripts/run_benchmarks.py` (new)
- `scripts/demo_multimodal.py` (new - demo script)

## QA Results
_(To be populated by QA agent after implementation)_
