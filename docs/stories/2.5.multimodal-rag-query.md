# Story 2.5: Multi-Modal RAG Query Implementation

## Status
Draft

## Story
**As a** user of the RAG system,
**I want** to search across text, images, and code with a single query,
**so that** I get comprehensive results including visual content.

## Acceptance Criteria

### Functional Requirements
1. Single query returns relevant text and image results
2. Results properly ranked across modalities (text, images, code)
3. MCP tool `perform_rag_query` returns image URLs/references in results
4. Multi-modal search configurable via USE_MULTIMODAL_RAG flag
5. Performance metrics for multi-modal queries logged

### Integration Requirements
6. Extension of existing `perform_rag_query` function in `src/crawl4ai_mcp.py`
7. Vector search across both text and image embeddings
8. Integration with existing RAG strategies (contextual, hybrid, agentic)
9. Backward compatible with text-only queries when multi-modal disabled

### Quality Requirements
10. Multi-modal queries complete within 2 seconds (p95 latency)
11. Comprehensive tests for multi-modal query functionality
12. Documentation updated with multi-modal query examples
13. No performance regression for text-only queries

## Tasks / Subtasks

- [ ] **Extend perform_rag_query Function** (AC: 1, 6)
  - [ ] Modify `perform_rag_query()` in `src/crawl4ai_mcp.py`
  - [ ] Add multi-modal query flag parameter
  - [ ] Add image result limit parameter
  - [ ] Implement parallel text and image vector searches
  - [ ] Merge results from multiple modalities
  - [ ] Maintain backward compatibility with text-only mode

- [ ] **Implement Image Vector Search** (AC: 1, 7)
  - [ ] Add image embedding query generation
  - [ ] Implement vector similarity search for images
  - [ ] Support for all database providers (Supabase, SQLite, Neo4j, etc.)
  - [ ] Add image result filtering (by source, format, size)
  - [ ] Implement image result ranking by similarity score
  - [ ] Handle cases where no images exist for query

- [ ] **Implement Cross-Modal Result Merging** (AC: 2)
  - [ ] Design result structure for multi-modal responses
  - [ ] Merge text and image results by similarity scores
  - [ ] Implement initial ranking strategy (normalized scores)
  - [ ] Group related content (text + related images)
  - [ ] Implement result deduplication (same image multiple times)
  - [ ] Preserve result provenance (source page, modality)

- [ ] **Result Structure Design** (AC: 3)
  - [ ] Define multi-modal result schema
  - [ ] Include text results with metadata
  - [ ] Include image results with URLs and storage paths
  - [ ] Include code results (if USE_AGENTIC_RAG enabled)
  - [ ] Add modality field to each result
  - [ ] Add relevance scores per modality

- [ ] **Configuration System** (AC: 4, 9)
  - [ ] Add `USE_MULTIMODAL_RAG` environment variable
  - [ ] Add `MULTIMODAL_MAX_TEXT_RESULTS` configuration
  - [ ] Add `MULTIMODAL_MAX_IMAGE_RESULTS` configuration
  - [ ] Add `MULTIMODAL_MAX_CODE_RESULTS` configuration
  - [ ] Add `MULTIMODAL_INCLUDE_IMAGES_IN_TEXT_RESULTS` flag
  - [ ] Update `.env.example` with multi-modal config

- [ ] **Integration with Existing RAG Strategies** (AC: 8)
  - [ ] Support contextual embeddings for multi-modal queries
  - [ ] Support hybrid search (vector + keyword) for images
  - [ ] Support agentic RAG with code + images
  - [ ] Support reranking across modalities (Story 2.6 dependency)
  - [ ] Support knowledge graph filtering if enabled
  - [ ] Ensure all strategies work with multi-modal enabled

- [ ] **Database Provider Integration** (AC: 7)
  - [ ] Update Supabase provider for multi-modal queries
  - [ ] Update SQLite provider for multi-modal queries
  - [ ] Update Neo4j provider for multi-modal queries
  - [ ] Update Pinecone provider for multi-modal queries
  - [ ] Update Weaviate provider for multi-modal queries
  - [ ] Add unified multi-modal query interface to base provider

- [ ] **Performance Optimization** (AC: 10, 13)
  - [ ] Implement parallel vector searches (text + images)
  - [ ] Add caching for frequently queried embeddings
  - [ ] Optimize result merging algorithm
  - [ ] Add query timeout handling
  - [ ] Implement result streaming for large result sets
  - [ ] Add performance profiling and metrics

- [ ] **Performance Monitoring** (AC: 5, 10)
  - [ ] Add timing metrics for multi-modal queries
  - [ ] Log separate timings for text/image/code searches
  - [ ] Track result counts per modality
  - [ ] Monitor cache hit rates
  - [ ] Add performance dashboard metrics
  - [ ] Alert on p95 latency > 2s threshold

- [ ] **MCP Tool Updates** (AC: 3)
  - [ ] Update `perform_rag_query` tool signature
  - [ ] Add `include_images` parameter
  - [ ] Add `include_code` parameter
  - [ ] Update tool description and examples
  - [ ] Update response schema documentation
  - [ ] Ensure backward compatibility

- [ ] **Testing Implementation** (AC: 11, 13)
  - [ ] Create `tests/test_multimodal_rag_query.py`
  - [ ] Unit tests for image vector search
  - [ ] Unit tests for result merging
  - [ ] Integration tests for multi-modal queries
  - [ ] Test with all database providers
  - [ ] Test performance (< 2s latency)
  - [ ] Test backward compatibility (text-only)
  - [ ] Test edge cases (no images, no text results)

- [ ] **Documentation Updates** (AC: 12)
  - [ ] Update `CLAUDE.md` with multi-modal query configuration
  - [ ] Add multi-modal query examples
  - [ ] Document result structure and fields
  - [ ] Add troubleshooting guide
  - [ ] Document performance tuning options

## Dev Notes

### Existing System Integration

**Integration Points:**
- **MCP Server**: `src/crawl4ai_mcp.py` - `perform_rag_query()` function
- **Database Providers**: `src/database/providers/` - Add multi-modal query methods
- **Utils**: `src/utils_refactored.py` - Embedding generation and chunking
- **RAG Strategies**: Existing strategies (contextual, hybrid, agentic, reranking)

**Technology Stack:**
- Python 3.13 with async/await
- FastMCP for MCP server
- Existing vector database abstraction
- Existing AI provider system
- Existing RAG strategy implementations

### Architecture Context

**Current RAG Query Flow (Text-Only):**
```
User Query →
1. Generate query embedding →
2. Vector search in crawled_pages →
3. Retrieve top-k chunks →
4. Apply reranking (if enabled) →
5. Format and return results
```

**Enhanced Multi-Modal RAG Query Flow:**
```
User Query →
1. Generate query embedding (text) →
2. Parallel Vector Searches:
   ├─ Search crawled_pages (text chunks)
   ├─ Search images (if USE_MULTIMODAL_RAG)
   └─ Search code_examples (if USE_AGENTIC_RAG)
3. Merge Results:
   ├─ Normalize similarity scores
   ├─ Initial ranking by score
   └─ Group related content
4. Cross-Modal Reranking (Story 2.6):
   ├─ Rerank across modalities
   └─ Apply ranking weights
5. Format Multi-Modal Response:
   ├─ Text results with metadata
   ├─ Image results with URLs
   └─ Code results with context
6. Return unified results
```

**Multi-Modal Result Structure:**
```python
{
    "query": "user query string",
    "results": [
        {
            "modality": "text",
            "content": "chunk of text...",
            "source": "https://example.com/page",
            "similarity_score": 0.85,
            "rank": 1,
            "metadata": {
                "page_title": "...",
                "section": "..."
            }
        },
        {
            "modality": "image",
            "image_url": "https://example.com/image.png",
            "storage_path": "/app/data/images/2025/10/01/abc123.png",
            "description": "Architecture diagram showing...",
            "extracted_text": "API Gateway -> Service...",
            "similarity_score": 0.82,
            "rank": 2,
            "metadata": {
                "source_page": "https://example.com/page",
                "format": "png",
                "dimensions": {"width": 800, "height": 600},
                "alt_text": "System architecture"
            }
        },
        {
            "modality": "code",
            "code": "def authenticate(user):\n    ...",
            "language": "python",
            "summary": "Authentication function example",
            "similarity_score": 0.79,
            "rank": 3,
            "metadata": {
                "source_page": "https://example.com/docs",
                "complexity": 15,
                "imports": ["jwt", "bcrypt"]
            }
        }
    ],
    "performance_metrics": {
        "total_time_ms": 1250,
        "text_search_time_ms": 450,
        "image_search_time_ms": 380,
        "code_search_time_ms": 220,
        "ranking_time_ms": 200,
        "text_result_count": 8,
        "image_result_count": 3,
        "code_result_count": 2
    }
}
```

### Key Technical Decisions

**1. Parallel Vector Searches:**
- Execute text, image, and code searches concurrently
- Use `asyncio.gather()` for parallel execution
- Reduce latency by avoiding sequential searches
- Handle partial failures gracefully

**2. Normalized Similarity Scores:**
- Different modalities may have different score scales
- Normalize all scores to 0-1 range
- Enable fair comparison across modalities
- Preserve original scores in metadata

**3. Result Grouping:**
- Group images with related text chunks from same page
- Group code examples with related documentation
- Preserve context relationships
- Enable "show related content" UI patterns

**4. Configurable Modality Mix:**
- Users can control which modalities to include
- Default: text + images (if multi-modal enabled)
- Optional: code (if agentic RAG enabled)
- Enable different result mixes for different use cases

**5. Backward Compatibility:**
- `USE_MULTIMODAL_RAG=false` → text-only (existing behavior)
- `USE_MULTIMODAL_RAG=true` → includes images
- Existing queries work unchanged
- New parameters optional

### Configuration Examples

```bash
# Enable multi-modal RAG
USE_MULTIMODAL_RAG=true

# Result limits per modality
MULTIMODAL_MAX_TEXT_RESULTS=10
MULTIMODAL_MAX_IMAGE_RESULTS=5
MULTIMODAL_MAX_CODE_RESULTS=3

# Result grouping
MULTIMODAL_GROUP_RELATED_CONTENT=true
MULTIMODAL_INCLUDE_IMAGES_IN_TEXT_RESULTS=true  # Embed related images in text results

# Performance tuning
MULTIMODAL_QUERY_TIMEOUT_MS=5000
MULTIMODAL_PARALLEL_SEARCHES=true
MULTIMODAL_CACHE_EMBEDDINGS=true

# Ranking (basic - enhanced in Story 2.6)
MULTIMODAL_INITIAL_RANKING_STRATEGY=score_based  # or interleaved, grouped

# Modality control
MULTIMODAL_DEFAULT_INCLUDE_IMAGES=true
MULTIMODAL_DEFAULT_INCLUDE_CODE=true  # Requires USE_AGENTIC_RAG

# Backward compatibility
# USE_MULTIMODAL_RAG=false  # Text-only (default if not set)
```

### Implementation Guidelines

**Parallel Vector Search Implementation:**
```python
async def perform_multimodal_vector_search(
    query: str,
    query_embedding: List[float],
    config: MultiModalConfig
) -> dict:
    """
    Execute parallel vector searches across modalities

    Returns:
    {
        "text_results": [...],
        "image_results": [...],
        "code_results": [...],
        "timings": {...}
    }
    """
    import asyncio
    from time import perf_counter

    # Prepare search tasks
    tasks = []
    timings = {}

    # Text search
    if config.include_text:
        start = perf_counter()
        text_task = asyncio.create_task(
            search_text_vectors(query_embedding, config.max_text_results)
        )
        tasks.append(("text", text_task, start))

    # Image search
    if config.include_images and config.multimodal_enabled:
        start = perf_counter()
        image_task = asyncio.create_task(
            search_image_vectors(query_embedding, config.max_image_results)
        )
        tasks.append(("image", image_task, start))

    # Code search
    if config.include_code and config.agentic_rag_enabled:
        start = perf_counter()
        code_task = asyncio.create_task(
            search_code_vectors(query_embedding, config.max_code_results)
        )
        tasks.append(("code", code_task, start))

    # Execute all searches in parallel
    results = {}
    for modality, task, start_time in tasks:
        try:
            results[f"{modality}_results"] = await task
            timings[f"{modality}_search_time_ms"] = (perf_counter() - start_time) * 1000
        except Exception as e:
            logger.error(f"{modality} search failed: {e}")
            results[f"{modality}_results"] = []
            timings[f"{modality}_search_time_ms"] = 0

    results["timings"] = timings
    return results
```

**Result Merging and Ranking:**
```python
def merge_multimodal_results(
    text_results: List[dict],
    image_results: List[dict],
    code_results: List[dict],
    strategy: str = "score_based"
) -> List[dict]:
    """
    Merge and rank results across modalities

    Strategies:
    - score_based: Sort by normalized similarity score
    - interleaved: Alternate between modalities
    - grouped: Group by source page
    """
    # Normalize similarity scores
    all_results = []

    for result in text_results:
        all_results.append({
            **result,
            "modality": "text",
            "normalized_score": _normalize_score(result["similarity_score"], "text")
        })

    for result in image_results:
        all_results.append({
            **result,
            "modality": "image",
            "normalized_score": _normalize_score(result["similarity_score"], "image")
        })

    for result in code_results:
        all_results.append({
            **result,
            "modality": "code",
            "normalized_score": _normalize_score(result["similarity_score"], "code")
        })

    # Apply ranking strategy
    if strategy == "score_based":
        # Sort by normalized score
        ranked = sorted(all_results, key=lambda x: x["normalized_score"], reverse=True)

    elif strategy == "interleaved":
        # Alternate between modalities
        ranked = _interleave_results(all_results)

    elif strategy == "grouped":
        # Group by source page
        ranked = _group_by_source(all_results)

    # Add rank field
    for i, result in enumerate(ranked):
        result["rank"] = i + 1

    return ranked

def _normalize_score(score: float, modality: str) -> float:
    """
    Normalize similarity scores across modalities

    Different embedding models may produce different score scales
    Normalize to 0-1 range for fair comparison
    """
    # Apply modality-specific normalization
    # This may need tuning based on actual score distributions
    if modality == "text":
        # Text embeddings typically range 0.3-0.9
        return (score - 0.3) / 0.6
    elif modality == "image":
        # Image embeddings may have different range
        return (score - 0.2) / 0.7
    elif modality == "code":
        # Code embeddings similar to text
        return (score - 0.3) / 0.6

    return score  # Fallback
```

**Enhanced perform_rag_query Function:**
```python
@mcp_server.tool()
async def perform_rag_query(
    query: str,
    source_filter: Optional[str] = None,
    limit: int = 10,
    include_images: bool = True,  # NEW
    include_code: bool = True,  # NEW
    image_limit: int = 5  # NEW
) -> str:
    """
    Perform multi-modal RAG query

    NEW: Returns text, images, and code snippets

    Args:
        query: Search query string
        source_filter: Optional source URL filter
        limit: Max results per modality
        include_images: Include image results (requires USE_MULTIMODAL_RAG)
        include_code: Include code results (requires USE_AGENTIC_RAG)
        image_limit: Max image results

    Returns:
        JSON string with multi-modal results
    """
    start_time = perf_counter()

    # Check if multi-modal enabled
    multimodal_enabled = os.getenv("USE_MULTIMODAL_RAG", "false").lower() == "true"
    if not multimodal_enabled:
        include_images = False

    # Generate query embedding
    query_embedding = await create_embedding(query)

    # Parallel vector searches
    search_results = await perform_multimodal_vector_search(
        query=query,
        query_embedding=query_embedding,
        config=MultiModalConfig(
            include_text=True,
            include_images=include_images and multimodal_enabled,
            include_code=include_code and USE_AGENTIC_RAG,
            max_text_results=limit,
            max_image_results=image_limit,
            max_code_results=limit // 2
        )
    )

    # Merge results
    merged_results = merge_multimodal_results(
        text_results=search_results["text_results"],
        image_results=search_results.get("image_results", []),
        code_results=search_results.get("code_results", []),
        strategy="score_based"
    )

    # Apply cross-modal reranking (Story 2.6)
    if USE_RERANKING and multimodal_enabled:
        merged_results = await rerank_multimodal_results(query, merged_results)

    # Calculate total time
    total_time_ms = (perf_counter() - start_time) * 1000

    # Format response
    response = {
        "query": query,
        "results": merged_results,
        "performance_metrics": {
            "total_time_ms": total_time_ms,
            **search_results["timings"]
        }
    }

    return json.dumps(response, indent=2)
```

### Testing

#### Testing Standards
- **Test file location:** `tests/test_multimodal_rag_query.py`
- **Test frameworks:** pytest with asyncio support
- **Test data:** Mock database with text, images, and code
- **Coverage target:** 85%+ for multi-modal query code

#### Test Scenarios

1. **Multi-Modal Vector Search:**
   - Parallel search across text and images
   - Parallel search across all modalities (text + images + code)
   - Handle missing modalities gracefully
   - Test search timeouts
   - Test partial failures (one modality fails)

2. **Result Merging:**
   - Merge text and image results
   - Merge all three modalities
   - Normalize similarity scores
   - Apply different ranking strategies
   - Handle empty result sets

3. **MCP Tool Integration:**
   - Call `perform_rag_query` with multi-modal enabled
   - Test include_images parameter
   - Test include_code parameter
   - Test result structure and schema
   - Verify backward compatibility (multi-modal disabled)

4. **Database Provider Integration:**
   - Test with Supabase provider
   - Test with SQLite provider
   - Test with Neo4j provider
   - Test with Pinecone provider
   - Test with Weaviate provider

5. **Configuration:**
   - Enable/disable multi-modal RAG
   - Configure result limits per modality
   - Test default configurations
   - Test backward compatibility settings

6. **Performance:**
   - Measure query latency
   - Verify < 2s p95 latency
   - Test parallel search performance
   - Test caching effectiveness
   - Compare with text-only baseline

7. **Result Quality:**
   - Verify relevant images returned for visual queries
   - Verify code snippets for programming queries
   - Verify text results for general queries
   - Test result ranking quality

8. **Edge Cases:**
   - Query with no image results
   - Query with no text results
   - Query with no results at all
   - Very long queries
   - Queries with special characters

#### Example Test Structure

```python
import pytest
from src.crawl4ai_mcp import perform_rag_query, perform_multimodal_vector_search

@pytest.mark.asyncio
async def test_multimodal_vector_search():
    """Test parallel vector search across modalities"""
    query = "authentication flow diagram"
    query_embedding = await create_embedding(query)

    results = await perform_multimodal_vector_search(
        query=query,
        query_embedding=query_embedding,
        config=MultiModalConfig(include_text=True, include_images=True)
    )

    assert "text_results" in results
    assert "image_results" in results
    assert "timings" in results
    assert len(results["text_results"]) > 0
    assert len(results["image_results"]) > 0

@pytest.mark.asyncio
async def test_merge_multimodal_results():
    """Test merging results from multiple modalities"""
    text_results = [{"content": "text", "similarity_score": 0.8}]
    image_results = [{"image_url": "...", "similarity_score": 0.75}]

    merged = merge_multimodal_results(
        text_results=text_results,
        image_results=image_results,
        code_results=[],
        strategy="score_based"
    )

    assert len(merged) == 2
    assert merged[0]["modality"] in ["text", "image"]
    assert "normalized_score" in merged[0]
    assert "rank" in merged[0]

@pytest.mark.asyncio
async def test_perform_rag_query_multimodal():
    """Integration test: multi-modal RAG query"""
    # Enable multi-modal RAG
    os.environ["USE_MULTIMODAL_RAG"] = "true"

    result_json = await perform_rag_query(
        query="architecture diagram",
        include_images=True,
        include_code=False
    )

    result = json.loads(result_json)
    assert "results" in result
    assert "performance_metrics" in result
    # Should have both text and image results
    modalities = {r["modality"] for r in result["results"]}
    assert "text" in modalities
    assert "image" in modalities

@pytest.mark.asyncio
async def test_multimodal_query_performance():
    """Test multi-modal query meets performance requirements"""
    from time import perf_counter

    start = perf_counter()
    result_json = await perform_rag_query(
        query="test query",
        include_images=True,
        include_code=True
    )
    elapsed_ms = (perf_counter() - start) * 1000

    # Verify < 2s p95 latency
    assert elapsed_ms < 2000, f"Query took {elapsed_ms}ms (expected < 2000ms)"

@pytest.mark.asyncio
async def test_backward_compatibility_text_only():
    """Test backward compatibility with text-only mode"""
    # Disable multi-modal RAG
    os.environ["USE_MULTIMODAL_RAG"] = "false"

    result_json = await perform_rag_query(
        query="test query",
        include_images=True  # Should be ignored
    )

    result = json.loads(result_json)
    # Should only have text results
    modalities = {r["modality"] for r in result["results"]}
    assert modalities == {"text"}
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-01 | 1.0 | Initial story creation for multi-modal RAG query | Sarah (PO) |

## Dev Agent Record

### Agent Model Used
_(To be populated by dev agent during implementation)_

### Debug Log References
_(To be populated by dev agent during implementation)_

### Completion Notes List
_(To be populated by dev agent during implementation)_

### File List
**Expected Files to be Created/Modified:**
- `src/crawl4ai_mcp.py` (modified - enhance perform_rag_query)
- `src/multimodal/` (new directory)
- `src/multimodal/query.py` (new - multi-modal query logic)
- `src/multimodal/result_merger.py` (new - result merging)
- `src/multimodal/config.py` (new - multi-modal configuration)
- `src/database/base.py` (modified - add multi-modal query methods)
- `src/database/providers/supabase_provider.py` (modified)
- `src/database/providers/sqlite_provider.py` (modified)
- `src/database/providers/neo4j_provider.py` (modified)
- `src/database/providers/pinecone_provider.py` (modified)
- `src/database/providers/weaviate_provider.py` (modified)
- `tests/test_multimodal_rag_query.py` (new)
- `.env.example` (modified - add multi-modal config)
- `CLAUDE.md` (modified - add multi-modal query docs)

## QA Results
_(To be populated by QA agent after implementation)_
