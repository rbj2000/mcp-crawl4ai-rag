# Story 2.4: Enhanced Code Snippet Processing

## Status
Draft

## Story
**As a** developer searching code examples,
**I want** better code understanding and extraction,
**so that** I can find relevant code snippets more accurately.

## Acceptance Criteria

### Functional Requirements
1. Code snippets extracted with accurate language detection
2. Code metadata captured (complexity, dependencies, imports)
3. Code context (surrounding text/documentation) preserved for better relevance
4. Enhanced code search quality measurably improved over basic text search
5. Integration with existing code examples table (if USE_AGENTIC_RAG enabled)

### Integration Requirements
6. Backward compatible with existing code extraction in `src/utils_refactored.py`
7. Support for all database providers (Supabase, SQLite, Neo4j, etc.)
8. Configuration via environment variables (USE_ENHANCED_CODE_PROCESSING)
9. Works with existing code extraction or as standalone enhancement

### Quality Requirements
10. Code extraction adds < 10% overhead to crawl time
11. Language detection accuracy > 95% for common languages
12. Comprehensive tests for code extraction and metadata
13. Documentation updated with enhanced code processing configuration

## Tasks / Subtasks

- [ ] **Enhanced Code Block Extraction** (AC: 1, 6)
  - [ ] Extend code extraction in `src/utils_refactored.py`
  - [ ] Parse fenced code blocks with language hints (```python, ```javascript, etc.)
  - [ ] Parse indented code blocks (markdown fallback)
  - [ ] Extract inline code snippets (`code` tags)
  - [ ] Handle multiple code blocks per document
  - [ ] Preserve code block metadata (line numbers, file references)

- [ ] **Language Detection** (AC: 1, 11)
  - [ ] Implement language detection from fenced code block hints
  - [ ] Implement language detection via file extensions (if present)
  - [ ] Fallback to heuristic detection (syntax patterns, keywords)
  - [ ] Support for 20+ common languages (Python, JavaScript, Java, Go, etc.)
  - [ ] Add language confidence scores
  - [ ] Handle mixed-language code blocks

- [ ] **Code Metadata Extraction** (AC: 2)
  - [ ] Extract imports/includes from code
  - [ ] Detect function/class definitions
  - [ ] Calculate code complexity metrics (lines, cyclomatic complexity estimate)
  - [ ] Extract comments and docstrings
  - [ ] Identify external dependencies (libraries, packages)
  - [ ] Detect code patterns (async, error handling, etc.)

- [ ] **Syntax-Aware Parsing** (AC: 2)
  - [ ] Integrate with language-specific parsers (AST for Python, etc.)
  - [ ] Extract function signatures and parameters
  - [ ] Extract class definitions and methods
  - [ ] Identify code structure (modules, classes, functions)
  - [ ] Handle syntax errors gracefully (partial parsing)
  - [ ] Cache parsed AST for performance

- [ ] **Code Context Preservation** (AC: 3)
  - [ ] Capture preceding text (explanation before code)
  - [ ] Capture following text (explanation after code)
  - [ ] Capture section headings containing code
  - [ ] Preserve code block position in document
  - [ ] Link code to related content (diagrams, descriptions)
  - [ ] Store context in enhanced format for retrieval

- [ ] **Code Syntax Highlighting Metadata** (AC: 1)
  - [ ] Generate syntax highlighting metadata for rendering
  - [ ] Identify tokens (keywords, strings, comments, etc.)
  - [ ] Store formatting hints for display
  - [ ] Support for multiple syntax themes
  - [ ] Maintain original formatting (indentation, whitespace)

- [ ] **Integration with Code Examples Table** (AC: 5, 9)
  - [ ] Extend `code_examples` table schema with new metadata
  - [ ] Store enhanced metadata (language, complexity, dependencies)
  - [ ] Store code context fields
  - [ ] Maintain backward compatibility with existing USE_AGENTIC_RAG
  - [ ] Add migration script for existing code examples

- [ ] **Database Schema Updates** (AC: 5, 7)
  - [ ] Add fields to `code_examples` table
  - [ ] Add `language` (detected language)
  - [ ] Add `complexity_score` (estimated complexity)
  - [ ] Add `imports` (extracted dependencies)
  - [ ] Add `function_signatures` (JSON array)
  - [ ] Add `context_before` and `context_after` (text)
  - [ ] Create indexes for efficient code search
  - [ ] Update all database provider implementations

- [ ] **Configuration System** (AC: 8)
  - [ ] Add `USE_ENHANCED_CODE_PROCESSING` flag
  - [ ] Add `CODE_LANGUAGE_DETECTION_MIN_CONFIDENCE` threshold
  - [ ] Add `CODE_CONTEXT_CHARS_BEFORE` and `CODE_CONTEXT_CHARS_AFTER`
  - [ ] Add `SUPPORTED_CODE_LANGUAGES` (comma-separated list)
  - [ ] Add `USE_CODE_AST_PARSING` flag (enable/disable AST parsing)
  - [ ] Update `.env.example` with code processing config

- [ ] **Code Search Enhancement** (AC: 4)
  - [ ] Implement language-filtered code search
  - [ ] Implement dependency-based code search
  - [ ] Combine code metadata with semantic search
  - [ ] Add code complexity filtering
  - [ ] Add code example ranking by relevance and context

- [ ] **Testing Implementation** (AC: 12, 13)
  - [ ] Create `tests/test_enhanced_code_processing.py`
  - [ ] Unit tests for language detection
  - [ ] Unit tests for code metadata extraction
  - [ ] Unit tests for syntax-aware parsing
  - [ ] Unit tests for context preservation
  - [ ] Integration tests with database providers
  - [ ] Test performance overhead
  - [ ] Test backward compatibility

- [ ] **Documentation Updates** (AC: 13)
  - [ ] Update `CLAUDE.md` with enhanced code processing configuration
  - [ ] Document supported languages and detection accuracy
  - [ ] Add code search examples and use cases
  - [ ] Document code metadata schema
  - [ ] Create troubleshooting guide for code extraction issues

## Dev Notes

### Existing System Integration

**Integration Points:**
- **Code Extraction**: `src/utils_refactored.py` - `extract_code_examples()` function
- **Database**: `code_examples` table (if USE_AGENTIC_RAG enabled)
- **Crawler**: `src/crawl4ai_mcp.py` - Code extraction during crawl
- **MCP Tools**: `search_code_examples` tool (existing)

**Technology Stack:**
- Python 3.13 with async/await
- Markdown parsing (existing Crawl4AI patterns)
- Language detection: Pygments or tree-sitter for syntax parsing
- AST parsing: Python's `ast` module, JavaScript parsers, etc.
- Pattern matching: Regex for language-specific patterns

### Architecture Context

**Current Code Extraction (Story 1.0 - USE_AGENTIC_RAG):**
```
Markdown Content →
1. Extract code blocks (```...```) →
2. Generate summary via LLM →
3. Store in code_examples table:
   - code (text)
   - summary (text)
   - embedding (vector)
```

**Enhanced Code Processing:**
```
Markdown Content →
1. Extract code blocks with metadata →
2. Language Detection →
   ├─ From fence hints (```python)
   ├─ From file extensions
   └─ Heuristic detection
3. Metadata Extraction →
   ├─ Imports/dependencies
   ├─ Function signatures
   ├─ Complexity metrics
   └─ Code structure
4. Context Preservation →
   ├─ Surrounding text
   ├─ Section headings
   └─ Position in document
5. Store Enhanced Data →
   ├─ code (text)
   ├─ language (string)
   ├─ complexity_score (int)
   ├─ imports (JSON array)
   ├─ function_signatures (JSON array)
   ├─ context_before (text)
   ├─ context_after (text)
   ├─ summary (text, existing)
   └─ embedding (vector, existing)
```

**Code Metadata Schema:**
```python
# Enhanced code_examples table
code_example = {
    "id": "uuid",
    "source_page_id": "uuid",
    "code": "string",  # Existing
    "language": "string",  # NEW
    "language_confidence": "float",  # NEW
    "complexity_score": "int",  # NEW
    "line_count": "int",  # NEW
    "imports": ["json", "array"],  # NEW - ["os", "sys", "requests"]
    "function_signatures": ["json", "array"],  # NEW - [{"name": "foo", "params": ["x", "y"]}]
    "class_names": ["json", "array"],  # NEW - ["MyClass", "Helper"]
    "context_before": "string",  # NEW - Text before code block
    "context_after": "string",  # NEW - Text after code block
    "position_in_page": "int",  # NEW - Ordinal position
    "summary": "string",  # Existing
    "embedding": "vector",  # Existing
}
```

### Key Technical Decisions

**1. Language Detection Strategy:**
```
Priority 1: Fenced code block hint (```python)
Priority 2: File extension in comment (# filename: script.py)
Priority 3: Pygments lexer detection (syntax-based)
Priority 4: Keyword/pattern heuristics (import, def, class, etc.)
Fallback: "unknown" language
```

**2. Metadata Extraction Approach:**
- **Lightweight**: Regex patterns for imports, function definitions
- **Advanced**: AST parsing for Python, tree-sitter for other languages
- **Configurable**: Enable/disable AST parsing via flag
- **Graceful Degradation**: Skip AST on parse errors

**3. Context Preservation:**
- Capture 200 chars before/after code block (configurable)
- Include section heading if within 500 chars
- Store markdown structure context (list item, quote, etc.)
- Preserve document position for ranking

**4. Performance Optimization:**
- Cache language detection results
- Lazy AST parsing (only when needed)
- Skip complex parsing for very small code snippets
- Parallel processing for multiple code blocks

**5. Backward Compatibility:**
- New fields optional (nullable)
- Existing code extraction works unchanged
- Enhanced processing opt-in via flag
- Migration script for existing data

### Configuration Examples

```bash
# Enable enhanced code processing
USE_ENHANCED_CODE_PROCESSING=true

# Language detection
CODE_LANGUAGE_DETECTION_MIN_CONFIDENCE=0.7
SUPPORTED_CODE_LANGUAGES=python,javascript,java,go,rust,typescript,bash,sql,html,css

# Metadata extraction
USE_CODE_AST_PARSING=true  # Enable AST parsing for Python
CODE_EXTRACT_IMPORTS=true
CODE_EXTRACT_FUNCTIONS=true
CODE_EXTRACT_COMPLEXITY=true

# Context preservation
CODE_CONTEXT_CHARS_BEFORE=200
CODE_CONTEXT_CHARS_AFTER=200
CODE_INCLUDE_SECTION_HEADING=true

# Performance tuning
CODE_MIN_LENGTH_FOR_EXTRACTION=10  # Skip very small snippets
CODE_MAX_LENGTH_FOR_AST=10000  # Skip AST for huge files
CODE_PARALLEL_PROCESSING=true

# Backward compatibility
USE_AGENTIC_RAG=true  # Must be enabled for code examples table
```

### Implementation Guidelines

**Language Detection Implementation:**
```python
import pygments
from pygments.lexers import guess_lexer, get_lexer_by_name

def detect_code_language(
    code: str,
    fence_hint: str = None,
    filename: str = None
) -> tuple[str, float]:
    """
    Detect programming language with confidence score

    Returns: (language, confidence)
    """
    # Priority 1: Fence hint
    if fence_hint:
        try:
            lexer = get_lexer_by_name(fence_hint)
            return lexer.name.lower(), 1.0
        except:
            pass

    # Priority 2: Filename extension
    if filename:
        try:
            lexer = guess_lexer_for_filename(filename, code)
            return lexer.name.lower(), 0.95
        except:
            pass

    # Priority 3: Syntax-based detection
    try:
        lexer = guess_lexer(code)
        # Calculate confidence based on token matches
        confidence = _calculate_lexer_confidence(lexer, code)
        return lexer.name.lower(), confidence
    except:
        pass

    # Priority 4: Keyword heuristics
    language, confidence = _detect_via_keywords(code)
    if language:
        return language, confidence

    return "unknown", 0.0
```

**Code Metadata Extraction:**
```python
def extract_code_metadata(code: str, language: str) -> dict:
    """
    Extract metadata from code snippet

    Returns:
    {
        "imports": [...],
        "function_signatures": [...],
        "class_names": [...],
        "complexity_score": int,
        "line_count": int
    }
    """
    metadata = {
        "imports": [],
        "function_signatures": [],
        "class_names": [],
        "complexity_score": 0,
        "line_count": len(code.splitlines())
    }

    # Language-specific extraction
    if language == "python":
        metadata.update(_extract_python_metadata(code))
    elif language in ["javascript", "typescript"]:
        metadata.update(_extract_js_metadata(code))
    elif language == "java":
        metadata.update(_extract_java_metadata(code))
    # ... other languages

    # Calculate complexity
    metadata["complexity_score"] = _calculate_complexity(code, language)

    return metadata

def _extract_python_metadata(code: str) -> dict:
    """Extract Python-specific metadata using AST"""
    import ast

    metadata = {
        "imports": [],
        "function_signatures": [],
        "class_names": []
    }

    try:
        tree = ast.parse(code)

        # Extract imports
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    metadata["imports"].append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                metadata["imports"].append(node.module)

        # Extract functions
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                params = [arg.arg for arg in node.args.args]
                metadata["function_signatures"].append({
                    "name": node.name,
                    "params": params
                })

        # Extract classes
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                metadata["class_names"].append(node.name)

    except SyntaxError:
        # Graceful degradation - use regex fallback
        metadata["imports"] = _extract_python_imports_regex(code)

    return metadata
```

**Context Preservation:**
```python
def extract_code_context(
    markdown_content: str,
    code_start_pos: int,
    code_end_pos: int,
    chars_before: int = 200,
    chars_after: int = 200
) -> dict:
    """
    Extract context around code block

    Returns:
    {
        "context_before": "...",
        "context_after": "...",
        "section_heading": "...",
        "position_in_page": int
    }
    """
    # Extract preceding text
    context_start = max(0, code_start_pos - chars_before)
    context_before = markdown_content[context_start:code_start_pos].strip()

    # Extract following text
    context_end = min(len(markdown_content), code_end_pos + chars_after)
    context_after = markdown_content[code_end_pos:context_end].strip()

    # Find section heading
    section_heading = _find_nearest_heading(
        markdown_content,
        code_start_pos
    )

    # Calculate position
    position_in_page = markdown_content[:code_start_pos].count("```") // 2

    return {
        "context_before": context_before,
        "context_after": context_after,
        "section_heading": section_heading,
        "position_in_page": position_in_page
    }
```

**Enhanced Code Search:**
```python
async def search_code_examples_enhanced(
    query: str,
    language: str = None,
    min_complexity: int = None,
    max_complexity: int = None,
    required_imports: list[str] = None,
    limit: int = 10
) -> list[dict]:
    """
    Enhanced code search with metadata filtering

    Combines:
    1. Semantic vector search (existing)
    2. Language filtering (new)
    3. Complexity filtering (new)
    4. Dependency filtering (new)
    5. Context-based ranking (new)
    """
    # Base semantic search
    results = await _vector_search_code_examples(query, limit * 2)

    # Apply metadata filters
    filtered = []
    for result in results:
        # Language filter
        if language and result["language"] != language:
            continue

        # Complexity filter
        if min_complexity and result["complexity_score"] < min_complexity:
            continue
        if max_complexity and result["complexity_score"] > max_complexity:
            continue

        # Import filter
        if required_imports:
            code_imports = set(result["imports"])
            if not all(imp in code_imports for imp in required_imports):
                continue

        filtered.append(result)

    # Re-rank by context relevance
    ranked = _rank_by_context(query, filtered)

    return ranked[:limit]
```

### Testing

#### Testing Standards
- **Test file location:** `tests/test_enhanced_code_processing.py`
- **Test frameworks:** pytest with asyncio support
- **Test data:** Code snippets in multiple languages
- **Coverage target:** 85%+ for code processing code

#### Test Scenarios

1. **Code Block Extraction:**
   - Extract fenced code blocks with language hints
   - Extract indented code blocks
   - Extract inline code
   - Handle multiple code blocks per document
   - Handle malformed code blocks

2. **Language Detection:**
   - Detect from fence hints (```python)
   - Detect from file extensions
   - Detect via syntax analysis (Pygments)
   - Detect via keyword heuristics
   - Handle unknown languages
   - Verify >95% accuracy for common languages

3. **Code Metadata Extraction:**
   - Extract imports (Python, JavaScript, Java)
   - Extract function signatures
   - Extract class definitions
   - Calculate complexity metrics
   - Handle syntax errors gracefully

4. **AST Parsing:**
   - Parse Python code with AST
   - Extract functions, classes, imports
   - Handle syntax errors
   - Performance for large files

5. **Context Preservation:**
   - Capture text before/after code
   - Find section headings
   - Preserve document position
   - Handle edge cases (code at start/end)

6. **Database Integration:**
   - Store enhanced metadata in database
   - Query by language
   - Query by complexity
   - Query by dependencies
   - Test all database providers

7. **Enhanced Search:**
   - Search with language filter
   - Search with complexity filter
   - Search with dependency filter
   - Verify improved relevance

8. **Performance:**
   - Measure extraction overhead
   - Verify <10% crawl time increase
   - Test parallel processing
   - Test caching

9. **Backward Compatibility:**
   - Existing code extraction still works
   - New fields optional
   - Migration script for existing data
   - USE_AGENTIC_RAG compatibility

#### Example Test Structure

```python
import pytest
from src.utils_refactored import extract_code_examples, detect_code_language

def test_language_detection_from_fence_hint():
    """Test language detection from fenced code block"""
    code = "def hello(): print('world')"
    language, confidence = detect_code_language(code, fence_hint="python")
    assert language == "python"
    assert confidence == 1.0

def test_language_detection_via_syntax():
    """Test language detection via syntax analysis"""
    code = "import os\ndef main():\n    pass"
    language, confidence = detect_code_language(code)
    assert language == "python"
    assert confidence > 0.9

@pytest.mark.asyncio
async def test_extract_python_metadata():
    """Test Python metadata extraction via AST"""
    code = """
import os
import sys

def process_data(x, y):
    return x + y

class DataProcessor:
    pass
    """
    metadata = extract_code_metadata(code, "python")
    assert "os" in metadata["imports"]
    assert "sys" in metadata["imports"]
    assert len(metadata["function_signatures"]) == 1
    assert metadata["function_signatures"][0]["name"] == "process_data"
    assert "DataProcessor" in metadata["class_names"]

@pytest.mark.asyncio
async def test_code_context_preservation():
    """Test extracting context around code blocks"""
    markdown = """
# Example Section
Here's how to process data:

```python
def process(x):
    return x * 2
```

This function doubles the input.
    """
    # Extract code and context
    # Verify context_before includes "Here's how to process data"
    # Verify context_after includes "This function doubles"
    # Verify section_heading is "Example Section"

@pytest.mark.asyncio
async def test_enhanced_code_search():
    """Test enhanced code search with metadata filters"""
    # Search for Python code with specific imports
    results = await search_code_examples_enhanced(
        query="data processing",
        language="python",
        required_imports=["pandas"],
        min_complexity=10
    )
    # Verify results filtered correctly
    # Verify all results are Python
    # Verify all results have pandas import
    # Verify all results meet complexity threshold

def test_backward_compatibility():
    """Test that existing code extraction still works"""
    # Test with USE_ENHANCED_CODE_PROCESSING=false
    # Verify basic extraction works
    # Verify no new fields required
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-01 | 1.0 | Initial story creation for enhanced code processing | Sarah (PO) |

## Dev Agent Record

### Agent Model Used
_(To be populated by dev agent during implementation)_

### Debug Log References
_(To be populated by dev agent during implementation)_

### Completion Notes List
_(To be populated by dev agent during implementation)_

### File List
**Expected Files to be Created/Modified:**
- `src/utils_refactored.py` (modified - enhanced code extraction)
- `src/code_processing/` (new directory)
- `src/code_processing/language_detection.py` (new)
- `src/code_processing/metadata_extraction.py` (new)
- `src/code_processing/context_extraction.py` (new)
- `src/code_processing/parsers/` (new directory for language parsers)
- `src/code_processing/parsers/python_parser.py` (new)
- `src/code_processing/parsers/javascript_parser.py` (new)
- `src/crawl4ai_mcp.py` (modified - integrate enhanced processing)
- `src/database/base.py` (modified - add code metadata methods)
- `src/database/providers/supabase_provider.py` (modified)
- `src/database/providers/sqlite_provider.py` (modified)
- `migrations/enhance_code_examples_table.sql` (new)
- `tests/test_enhanced_code_processing.py` (new)
- `tests/fixtures/code_examples/` (new - test code snippets)
- `.env.example` (modified - add code processing config)
- `CLAUDE.md` (modified - add enhanced code processing docs)
- `requirements.txt` (modified - add pygments, tree-sitter if needed)

## QA Results
_(To be populated by QA agent after implementation)_
