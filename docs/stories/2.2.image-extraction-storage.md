# Story 2.2: Image Extraction and Storage Infrastructure

## Status
Draft

## Story
**As a** developer using the crawler,
**I want** images automatically extracted and stored during web crawling,
**so that** visual content is available for multi-modal search.

## Acceptance Criteria

### Functional Requirements
1. Images extracted from crawled pages with metadata (dimensions, format, alt text, surrounding context)
2. Images stored efficiently with configurable storage backend (filesystem vs. blob storage)
3. Database schema supports image metadata storage
4. Image URLs and local storage paths properly tracked
5. Image-to-document relationship tracking maintained

### Integration Requirements
6. Extension of `crawl_single_page` to extract images without breaking existing functionality
7. Integration with existing database providers (Supabase, SQLite, Neo4j, Pinecone, Weaviate)
8. Configuration via environment variables (IMAGE_STORAGE_TYPE, IMAGE_STORAGE_PATH, etc.)
9. No breaking changes to existing crawl functionality

### Quality Requirements
10. Image processing adds < 30% overhead to crawl time
11. Comprehensive tests for image extraction and storage
12. Documentation updated with image storage configuration
13. No regression in existing text-only crawl functionality

## Tasks / Subtasks

- [ ] **Define Image Storage Architecture** (AC: 2, 8)
  - [ ] Design image storage interface (`ImageStorageProvider`)
  - [ ] Define filesystem storage implementation
  - [ ] Define blob storage interface (for future cloud storage)
  - [ ] Add configuration schema for storage backends
  - [ ] Document storage path conventions and naming

- [ ] **Implement Image Extraction from HTML** (AC: 1, 6)
  - [ ] Extend `crawl_single_page` in `src/crawl4ai_mcp.py`
  - [ ] Extract `<img>` tags from HTML content
  - [ ] Extract background images from CSS (optional)
  - [ ] Parse image metadata (alt text, title, dimensions from attributes)
  - [ ] Capture surrounding context (parent elements, nearby text)
  - [ ] Filter out tracking pixels and small images (configurable threshold)

- [ ] **Implement Image Download and Storage** (AC: 2, 4)
  - [ ] Create `src/storage/` directory structure
  - [ ] Implement `ImageStorageProvider` base class
  - [ ] Implement `FilesystemStorageProvider`
  - [ ] Implement async image download with retry logic
  - [ ] Generate unique image identifiers (hash-based or UUID)
  - [ ] Handle image format conversion if needed
  - [ ] Implement image size limits and validation

- [ ] **Database Schema Updates** (AC: 3, 5, 7)
  - [ ] Design `images` table schema
  - [ ] Add migration scripts for each database provider
  - [ ] Implement image-to-document relationship table
  - [ ] Add image metadata fields (URL, storage_path, dimensions, format, alt_text, etc.)
  - [ ] Create indexes for efficient image lookup
  - [ ] Update database provider interfaces to support image storage

- [ ] **Image Metadata Extraction** (AC: 1, 5)
  - [ ] Extract image dimensions (from HTML attributes or actual image)
  - [ ] Extract image format (JPEG, PNG, SVG, WebP, etc.)
  - [ ] Extract alt text and title attributes
  - [ ] Capture surrounding context (e.g., paragraph text, headings)
  - [ ] Generate image description placeholder (for Story 2.3)
  - [ ] Store image position in document (ordinal)

- [ ] **Configuration System** (AC: 8, 9)
  - [ ] Add image storage configuration to environment variables
  - [ ] Add `IMAGE_STORAGE_TYPE` (filesystem, blob_storage)
  - [ ] Add `IMAGE_STORAGE_PATH` for filesystem storage
  - [ ] Add `IMAGE_MAX_SIZE_MB` for size limits
  - [ ] Add `IMAGE_MIN_DIMENSIONS` to filter small images
  - [ ] Add `EXTRACT_IMAGES` flag to enable/disable feature
  - [ ] Update `.env.example` with image configuration

- [ ] **Database Provider Integration** (AC: 7)
  - [ ] Update Supabase provider with image table support
  - [ ] Update SQLite provider with image table support
  - [ ] Update Neo4j provider with image node support
  - [ ] Update Pinecone provider with image metadata support
  - [ ] Update Weaviate provider with image class support
  - [ ] Ensure backward compatibility (images optional)

- [ ] **Testing Implementation** (AC: 11, 13)
  - [ ] Create `tests/test_image_extraction.py`
  - [ ] Unit tests for HTML image extraction
  - [ ] Unit tests for image download and storage
  - [ ] Unit tests for metadata extraction
  - [ ] Integration tests with database providers
  - [ ] Test image storage path generation
  - [ ] Test graceful handling of missing/broken images
  - [ ] Performance tests for crawl overhead

- [ ] **Documentation Updates** (AC: 12)
  - [ ] Update `CLAUDE.md` with image storage configuration
  - [ ] Document database schema changes
  - [ ] Add image storage architecture documentation
  - [ ] Create migration guide for existing deployments
  - [ ] Document supported image formats and limitations

## Dev Notes

### Existing System Integration

**Integration Points:**
- **Crawler**: `src/crawl4ai_mcp.py` - `crawl_single_page()` function
- **Database Providers**: `src/database/providers/` - All database providers
- **Database Base**: `src/database/base.py` - Add image storage methods
- **Configuration**: Environment variable loading in lifespan manager

**Technology Stack:**
- Python 3.13 with async/await
- Crawl4AI for HTML parsing and content extraction
- aiohttp for async image downloads
- PIL/Pillow for image processing (dimensions, format detection)
- Existing database provider abstraction

### Architecture Context

**Current Crawl Pipeline:**
```
1. crawl_single_page(url, strategy) →
2. Crawl4AI extracts HTML/Markdown →
3. Content chunking and processing →
4. Embedding generation (text only) →
5. Storage in vector database
```

**Enhanced Crawl Pipeline with Images:**
```
1. crawl_single_page(url, strategy) →
2. Crawl4AI extracts HTML/Markdown →
3. Image Extraction (NEW):
   ├─ Parse <img> tags from HTML
   ├─ Download images asynchronously
   ├─ Extract metadata (alt text, dimensions, context)
   ├─ Store images (filesystem/blob)
   └─ Generate image records
4. Content chunking and processing (existing) →
5. Embedding generation (text + image placeholders) →
6. Storage in vector database (text + image metadata)
```

**Image Storage Architecture:**
```
ImageStorageProvider (ABC)
  ├─ async def store_image(url, image_data, metadata) → ImageRecord
  ├─ async def retrieve_image(image_id) → ImageRecord
  ├─ async def delete_image(image_id) → bool
  └─ def get_storage_path(image_id) → str

FilesystemStorageProvider(ImageStorageProvider)
  └─ Stores images in configured directory
  └─ Organizes by date or hash-based paths

BlobStorageProvider(ImageStorageProvider) - Future
  └─ Stores images in cloud blob storage (S3, GCS, Azure Blob)
```

### Database Schema

**New `images` Table:**
```sql
CREATE TABLE images (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT NOT NULL,
    storage_path TEXT NOT NULL,
    source_page_id UUID REFERENCES crawled_pages(id) ON DELETE CASCADE,
    format VARCHAR(10),  -- 'jpeg', 'png', 'svg', 'webp', etc.
    width INTEGER,
    height INTEGER,
    file_size_bytes INTEGER,
    alt_text TEXT,
    title TEXT,
    surrounding_context TEXT,  -- Text near the image
    position_in_page INTEGER,  -- Ordinal position
    extracted_text TEXT,  -- OCR text (Story 2.3)
    description TEXT,  -- AI-generated caption (Story 2.3)
    embedding vector(768),  -- Image embedding (Story 2.3)
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_images_source_page ON images(source_page_id);
CREATE INDEX idx_images_url ON images(url);
```

**SQLite Schema (simplified):**
```sql
CREATE TABLE images (
    id TEXT PRIMARY KEY,
    url TEXT NOT NULL,
    storage_path TEXT NOT NULL,
    source_page_id TEXT REFERENCES crawled_pages(id) ON DELETE CASCADE,
    format TEXT,
    width INTEGER,
    height INTEGER,
    file_size_bytes INTEGER,
    alt_text TEXT,
    title TEXT,
    surrounding_context TEXT,
    position_in_page INTEGER,
    extracted_text TEXT,
    description TEXT,
    embedding BLOB,  -- Serialized vector
    created_at TEXT DEFAULT (datetime('now')),
    updated_at TEXT DEFAULT (datetime('now'))
);
```

### Database Provider-Specific Considerations

**Supabase (PostgreSQL + pgvector):**
- Native support for `vector` type for embeddings
- Use `UUID` for primary keys with `gen_random_uuid()`
- Leverage foreign key constraints for image-to-page relationships
- Use `ON DELETE CASCADE` for automatic cleanup

**SQLite:**
- Use `TEXT` for UUIDs, `BLOB` for serialized vectors
- Implement manual foreign key enforcement if needed
- Consider separate vector extension (sqlite-vec) for similarity search
- Use `datetime('now')` for timestamps

**Neo4j:**
- Decision: Image nodes vs. properties on Page nodes
- **Recommended**: Create separate `Image` nodes with relationships
  ```cypher
  (:Page)-[:CONTAINS_IMAGE]->(:Image)
  ```
- Store image embeddings as node properties (vector type)
- Use vector index for similarity search
- Leverage graph relationships for context-aware search

**Pinecone:**
- Metadata-only approach (no binary image storage)
- Store image URL and storage_path in metadata
- Image embeddings stored as vectors (Story 2.3)
- Use namespace for image vectors vs. text vectors
- Leverage metadata filtering for image-specific queries

**Weaviate:**
- Create separate `Image` class with cross-references to `Page` class
- Store image metadata as properties
- Image embeddings handled by Weaviate's vectorizer (Story 2.3)
- Use multi-tenancy if needed for source separation

### Configuration Examples

```bash
# Enable image extraction
EXTRACT_IMAGES=true

# Storage configuration
IMAGE_STORAGE_TYPE=filesystem  # or blob_storage (future)
IMAGE_STORAGE_PATH=/app/data/images  # Filesystem storage location

# Image filtering
IMAGE_MIN_DIMENSIONS=100x100  # Filter out small images
IMAGE_MAX_SIZE_MB=10  # Skip very large images
ALLOWED_IMAGE_FORMATS=jpeg,png,webp,svg  # Comma-separated

# Image deduplication
IMAGE_DEDUPLICATION_STRATEGY=hash  # or url, or none
IMAGE_HASH_ALGORITHM=sha256  # For hash-based deduplication

# Performance tuning
IMAGE_DOWNLOAD_TIMEOUT=30  # Seconds
IMAGE_CONCURRENT_DOWNLOADS=5  # Max parallel downloads per page
```

### Implementation Guidelines

**Image Extraction Strategy:**
1. Parse HTML for `<img>` tags using BeautifulSoup or lxml
2. Extract `src`, `alt`, `title`, `width`, `height` attributes
3. Resolve relative URLs to absolute URLs
4. Capture surrounding context (e.g., parent `<figure>`, nearby `<p>` tags)
5. Filter out tracking pixels (1x1 images, common tracker domains)
6. Download images asynchronously (use semaphore to limit concurrency)

**Storage Path Convention:**
```
{IMAGE_STORAGE_PATH}/{year}/{month}/{day}/{image_hash}.{format}

Example:
/app/data/images/2025/09/30/a3b2c1d4e5f6.jpeg
```

**Error Handling:**
- Image download failure → log warning, continue crawl
- Invalid image format → skip, log warning
- Storage failure → log error, continue crawl
- Large image (> MAX_SIZE) → skip, log info

**Performance Optimization:**
- Limit concurrent image downloads (semaphore)
- Use HEAD request first to check image size before downloading
- Stream download for large images
- Implement download timeout
- Cache image metadata to avoid re-downloading

**Image Deduplication Strategy:**

Deduplication prevents storing and processing the same image multiple times:

```python
async def check_image_duplicate(url: str, image_data: bytes, strategy: str) -> Optional[str]:
    """
    Check if image already exists in database

    Strategies:
    - 'hash': Content-based deduplication using SHA256
    - 'url': URL-based lookup (faster but less accurate)
    - 'none': No deduplication (store all images)

    Returns: Existing image_id if duplicate found, None otherwise
    """
    if strategy == 'none':
        return None

    if strategy == 'url':
        # Fast URL-based lookup
        existing = await db.query_image_by_url(url)
        return existing.id if existing else None

    if strategy == 'hash':
        # Content-based hash for true deduplication
        import hashlib
        image_hash = hashlib.sha256(image_data).hexdigest()
        existing = await db.query_image_by_hash(image_hash)
        return existing.id if existing else None
```

**Deduplication Workflow:**
1. Extract image URL from HTML
2. Check for duplicate using configured strategy
3. If duplicate found:
   - Create new image-to-page relationship
   - Skip download and storage
   - Reuse existing embeddings (Story 2.3)
4. If not duplicate:
   - Download image
   - Store with unique identifier
   - Proceed with vision processing

**Benefits:**
- Reduces storage space (same image referenced by multiple pages)
- Reduces vision processing costs (embed once, reuse many times)
- Faster crawl times (skip duplicate downloads)

### Migration Scripts

**Migration Template Structure:**
```sql
-- Migration: Add images table
-- Version: 2.2.0
-- Author: Dev Agent
-- Date: 2025-09-30
-- Safe to run multiple times: Yes (uses IF NOT EXISTS)

-- Supabase/PostgreSQL
CREATE TABLE IF NOT EXISTS images (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT NOT NULL,
    storage_path TEXT NOT NULL,
    content_hash VARCHAR(64),  -- For deduplication
    source_page_id UUID REFERENCES crawled_pages(id) ON DELETE CASCADE,
    format VARCHAR(10),
    width INTEGER,
    height INTEGER,
    file_size_bytes INTEGER,
    alt_text TEXT,
    title TEXT,
    surrounding_context TEXT,
    position_in_page INTEGER,
    extracted_text TEXT,
    description TEXT,
    embedding vector(768),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_images_source_page ON images(source_page_id);
CREATE INDEX IF NOT EXISTS idx_images_url ON images(url);
CREATE INDEX IF NOT EXISTS idx_images_content_hash ON images(content_hash);
CREATE INDEX IF NOT EXISTS idx_images_embedding ON images USING ivfflat (embedding vector_cosine_ops);

-- Image-to-Page relationship table (for many-to-many if needed)
CREATE TABLE IF NOT EXISTS page_images (
    page_id UUID REFERENCES crawled_pages(id) ON DELETE CASCADE,
    image_id UUID REFERENCES images(id) ON DELETE CASCADE,
    position_in_page INTEGER,
    PRIMARY KEY (page_id, image_id)
);

-- Rollback script (saved separately as rollback_2.2.0.sql)
-- DROP TABLE IF EXISTS page_images;
-- DROP TABLE IF EXISTS images;
```

**SQLite Migration:**
```sql
-- Migration: Add images table (SQLite)
-- Version: 2.2.0

CREATE TABLE IF NOT EXISTS images (
    id TEXT PRIMARY KEY,
    url TEXT NOT NULL,
    storage_path TEXT NOT NULL,
    content_hash TEXT,
    source_page_id TEXT REFERENCES crawled_pages(id) ON DELETE CASCADE,
    format TEXT,
    width INTEGER,
    height INTEGER,
    file_size_bytes INTEGER,
    alt_text TEXT,
    title TEXT,
    surrounding_context TEXT,
    position_in_page INTEGER,
    extracted_text TEXT,
    description TEXT,
    embedding BLOB,
    created_at TEXT DEFAULT (datetime('now')),
    updated_at TEXT DEFAULT (datetime('now'))
);

CREATE INDEX IF NOT EXISTS idx_images_source_page ON images(source_page_id);
CREATE INDEX IF NOT EXISTS idx_images_url ON images(url);
CREATE INDEX IF NOT EXISTS idx_images_content_hash ON images(content_hash);
```

**Neo4j Migration (Cypher):**
```cypher
// Migration: Add Image nodes and relationships
// Version: 2.2.0

// Create constraint for unique image IDs
CREATE CONSTRAINT image_id_unique IF NOT EXISTS
FOR (i:Image) REQUIRE i.id IS UNIQUE;

// Create vector index for image embeddings (Neo4j 5.11+)
CREATE VECTOR INDEX image_embeddings IF NOT EXISTS
FOR (i:Image) ON (i.embedding)
OPTIONS {
  indexConfig: {
    `vector.dimensions`: 768,
    `vector.similarity_function`: 'cosine'
  }
};

// Example Image node creation (done by application, not migration)
// CREATE (i:Image {
//   id: 'uuid',
//   url: 'https://example.com/image.jpg',
//   storage_path: '/app/data/images/2025/09/30/hash.jpg',
//   content_hash: 'sha256hash',
//   format: 'jpeg',
//   width: 1920,
//   height: 1080
// });

// Example relationship creation
// MATCH (p:Page {id: 'page_uuid'})
// MATCH (i:Image {id: 'image_uuid'})
// CREATE (p)-[:CONTAINS_IMAGE {position: 1}]->(i);
```

### Testing

#### Testing Standards
- **Test file location:** `tests/test_image_extraction.py`
- **Test frameworks:** pytest with asyncio support
- **Test data:** Mock images and HTML fixtures
- **Coverage target:** 85%+ for new image extraction code

#### Test Scenarios
1. **Image Extraction:**
   - Extract images from HTML with various formats
   - Extract alt text and metadata
   - Handle relative and absolute URLs
   - Filter small images (< min dimensions)
   - Handle missing images (404, broken links)

2. **Image Download:**
   - Successful image download
   - Download timeout handling
   - Large image handling (size limits)
   - Network error handling
   - Concurrent download limits

3. **Image Storage:**
   - Filesystem storage success
   - Storage path generation
   - Duplicate image handling (same URL)
   - Storage failure handling

4. **Database Integration:**
   - Image metadata stored correctly
   - Image-to-page relationship maintained
   - Database schema migration
   - Query images by page ID

5. **Configuration:**
   - Enable/disable image extraction
   - Configure storage backend
   - Configure image filtering rules

6. **Performance:**
   - Measure crawl overhead with images
   - Verify < 30% overhead requirement
   - Test concurrent download limits

#### Example Test Structure
```python
import pytest
from src.crawl4ai_mcp import crawl_single_page
from src.storage.filesystem_storage import FilesystemStorageProvider

@pytest.mark.asyncio
async def test_image_extraction_from_html():
    """Test extracting images from HTML content"""
    # Mock HTML with images
    # Call image extraction function
    # Verify images extracted with metadata

@pytest.mark.asyncio
async def test_image_download_and_storage():
    """Test downloading and storing images"""
    # Mock image URL
    # Download image
    # Store in filesystem
    # Verify storage path and metadata

@pytest.mark.asyncio
async def test_crawl_with_image_extraction():
    """Integration test: crawl page with images"""
    # Mock web page with images
    # Crawl page with EXTRACT_IMAGES=true
    # Verify images stored in database
    # Verify no regression in text extraction
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-30 | 1.0 | Initial story creation for image extraction and storage | Sarah (PO) |
| 2025-10-01 | 1.1 | Added provider-specific notes, deduplication strategy, and migration scripts | Sarah (PO) |

## Dev Agent Record

### Agent Model Used
_(To be populated by dev agent during implementation)_

### Debug Log References
_(To be populated by dev agent during implementation)_

### Completion Notes List
_(To be populated by dev agent during implementation)_

### File List
**Expected Files to be Created/Modified:**
- `src/storage/__init__.py` (new)
- `src/storage/base.py` (new - ImageStorageProvider interface)
- `src/storage/filesystem_storage.py` (new)
- `src/crawl4ai_mcp.py` (modified - add image extraction)
- `src/database/base.py` (modified - add image storage methods)
- `src/database/providers/supabase_provider.py` (modified)
- `src/database/providers/sqlite_provider.py` (modified)
- `src/database/providers/neo4j_provider.py` (modified)
- `src/database/providers/pinecone_provider.py` (modified)
- `src/database/providers/weaviate_provider.py` (modified)
- `tests/test_image_extraction.py` (new)
- `migrations/add_images_table.sql` (new)
- `.env.example` (modified - add image config)
- `CLAUDE.md` (modified - add image storage docs)

## QA Results
_(To be populated by QA agent after implementation)_
