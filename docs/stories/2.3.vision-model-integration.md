# Story 2.3: Vision Model Integration for Image Understanding

## Status
Draft

## Story
**As a** user searching documentation,
**I want** images to have semantic descriptions and embeddings,
**so that** I can find relevant images using natural language queries.

## Acceptance Criteria

### Functional Requirements
1. Images have vector embeddings stored in vector database
2. Image descriptions automatically generated and indexed
3. OCR extracts text from screenshots/diagrams
4. Vision model failures gracefully degrade to metadata-only indexing
5. Performance metrics logged for vision model calls

### Integration Requirements
6. Image embedding generation via vLLM vision models
7. Integration with existing embedding pipeline from `src/utils_refactored.py`
8. Configuration via environment variables (GENERATE_IMAGE_CAPTIONS, EXTRACT_IMAGE_TEXT)
9. Support for multiple vision models (LLaVA, Qwen-VL, Phi-3-Vision)

### Quality Requirements
10. Vision model processing adds < 2 seconds per image (p95)
11. Comprehensive tests for vision model integration
12. Documentation updated with vision model configuration
13. No regression in existing text embedding functionality

## Tasks / Subtasks

- [ ] **Implement Image Embedding Generation** (AC: 1, 6)
  - [ ] Add `generate_image_embedding()` to `VisionProvider` interface
  - [ ] Implement image embedding in `VLLMProvider`
  - [ ] Add image preprocessing (resize, format conversion)
  - [ ] Integrate with database provider's vector storage
  - [ ] Add batch processing for multiple images
  - [ ] Implement caching for duplicate images

- [ ] **Implement Image Caption/Description Generation** (AC: 2, 6)
  - [ ] Add `generate_image_caption()` to `VisionProvider` interface
  - [ ] Implement caption generation in `VLLMProvider`
  - [ ] Design prompt templates for different image types (diagram, screenshot, photo)
  - [ ] Store captions in `images.description` field
  - [ ] Add caption quality validation
  - [ ] Implement fallback to alt text if caption generation fails

- [ ] **Implement OCR Text Extraction** (AC: 3, 9)
  - [ ] Add `extract_text_from_image()` to `VisionProvider` interface
  - [ ] Implement OCR via vision model (preferred) or Tesseract (fallback)
  - [ ] Store extracted text in `images.extracted_text` field
  - [ ] Combine extracted text with surrounding context for indexing
  - [ ] Handle multi-language text extraction
  - [ ] Filter out low-confidence OCR results

- [ ] **Integrate with Embedding Pipeline** (AC: 7)
  - [ ] Extend `src/utils_refactored.py` with image embedding functions
  - [ ] Add `create_image_embedding(image_data, provider)` function
  - [ ] Integrate with existing provider selection logic
  - [ ] Add image embedding batch processing
  - [ ] Update chunking logic to include image embeddings
  - [ ] Ensure backward compatibility with text-only embeddings

- [ ] **Vision Model Configuration** (AC: 8, 9)
  - [ ] Add vision model configuration to environment variables
  - [ ] Add `GENERATE_IMAGE_CAPTIONS` flag (true/false)
  - [ ] Add `EXTRACT_IMAGE_TEXT` flag (true/false)
  - [ ] Add `VISION_MODEL_TIMEOUT` configuration
  - [ ] Add `VISION_MODEL_MAX_RETRIES` configuration
  - [ ] Update `.env.example` with vision configuration

- [ ] **Graceful Degradation and Error Handling** (AC: 4)
  - [ ] Implement fallback to metadata-only indexing on vision model failure
  - [ ] Add retry logic with exponential backoff for transient errors
  - [ ] Log vision model errors with context
  - [ ] Continue crawl even if vision processing fails
  - [ ] Add health check for vision model availability
  - [ ] Implement circuit breaker pattern for repeated failures

- [ ] **Performance Monitoring and Optimization** (AC: 5, 10)
  - [ ] Add timing metrics for vision model calls
  - [ ] Log performance metrics (latency, success rate, error rate)
  - [ ] Implement async parallel processing for multiple images
  - [ ] Add image size limits to avoid excessive processing time
  - [ ] Implement timeout handling for slow vision model responses
  - [ ] Add performance dashboard metrics (if monitoring enabled)

- [ ] **Database Integration** (AC: 1, 2, 3)
  - [ ] Update database providers to store image embeddings
  - [ ] Add vector similarity search for image embeddings
  - [ ] Store image descriptions in `images.description` field
  - [ ] Store OCR text in `images.extracted_text` field
  - [ ] Create indexes for efficient image search
  - [ ] Test with all database providers (Supabase, SQLite, Neo4j, etc.)

- [ ] **Testing Implementation** (AC: 11, 13)
  - [ ] Create `tests/test_vision_model_integration.py`
  - [ ] Unit tests for image embedding generation
  - [ ] Unit tests for caption generation
  - [ ] Unit tests for OCR text extraction
  - [ ] Mock vision model responses for testing
  - [ ] Test graceful degradation on vision model failure
  - [ ] Test performance metrics logging
  - [ ] Integration tests with database providers
  - [ ] Test backward compatibility (text-only mode)

- [ ] **Documentation Updates** (AC: 12)
  - [ ] Update `CLAUDE.md` with vision model configuration
  - [ ] Document supported vision models and capabilities
  - [ ] Add troubleshooting guide for vision model issues
  - [ ] Document performance characteristics and tuning
  - [ ] Create example use cases (diagrams, screenshots, code images)

## Dev Notes

### Existing System Integration

**Integration Points:**
- **AI Provider System**: `src/ai_providers/providers/vllm_provider.py` - Add vision methods
- **Embedding Pipeline**: `src/utils_refactored.py` - Extend for image embeddings
- **Crawler**: `src/crawl4ai_mcp.py` - Call vision processing after image extraction
- **Database Providers**: `src/database/providers/` - Store image embeddings and descriptions

**Technology Stack:**
- Python 3.13 with async/await
- vLLM vision models (LLaVA, Qwen-VL, Phi-3-Vision)
- PIL/Pillow for image preprocessing
- Existing embedding pipeline patterns
- Vector database storage (pgvector, SQLite-vec, etc.)

### Architecture Context

**Vision Processing Pipeline:**
```
Image Extracted (Story 2.2) →
1. Preprocessing (resize, format conversion) →
2. Vision Model Processing:
   ├─ Generate Image Embedding (vector)
   ├─ Generate Caption/Description (text)
   └─ Extract Text via OCR (text)
3. Store Results:
   ├─ embedding → images.embedding (vector)
   ├─ description → images.description (text)
   └─ extracted_text → images.extracted_text (text)
4. Index for Search:
   ├─ Vector similarity search (embedding)
   └─ Full-text search (description + extracted_text)
```

**Vision Provider Interface Extension:**
```python
class VisionProvider(ABC):
    """Interface for vision model capabilities"""

    @abstractmethod
    async def generate_image_embedding(
        self,
        image_data: bytes,
        model: str = None
    ) -> List[float]:
        """Generate vector embedding for image"""
        pass

    @abstractmethod
    async def generate_image_caption(
        self,
        image_data: bytes,
        prompt_template: str = None,
        model: str = None
    ) -> str:
        """Generate natural language description of image"""
        pass

    @abstractmethod
    async def extract_text_from_image(
        self,
        image_data: bytes,
        model: str = None
    ) -> str:
        """Extract text from image via OCR"""
        pass

    def supports_vision(self) -> bool:
        """Check if provider supports vision capabilities"""
        return True
```

**VLLMProvider Vision Implementation:**
```python
class VLLMProvider(HybridAIProvider, VisionProvider):
    """vLLM provider with vision model support"""

    async def generate_image_embedding(
        self,
        image_data: bytes,
        model: str = None
    ) -> List[float]:
        """
        Generate image embedding using vLLM vision model

        Process:
        1. Preprocess image (resize, format)
        2. Send to vLLM endpoint with vision model
        3. Extract embedding vector from response
        4. Return normalized vector
        """
        model = model or self.config.vision_model

        # Preprocess image
        processed_image = self._preprocess_image(image_data)

        # Call vLLM vision endpoint
        response = await self._call_vision_endpoint(
            endpoint="/v1/embeddings",
            image=processed_image,
            model=model
        )

        return response["embedding"]

    async def generate_image_caption(
        self,
        image_data: bytes,
        prompt_template: str = None,
        model: str = None
    ) -> str:
        """
        Generate caption using vision model

        Prompt templates:
        - diagram: "Describe this technical diagram in detail"
        - screenshot: "Describe what is shown in this screenshot"
        - code: "Describe the code shown in this image"
        - photo: "Describe this image"
        """
        model = model or self.config.vision_model
        prompt = prompt_template or "Describe this image in detail"

        # Preprocess image
        processed_image = self._preprocess_image(image_data)

        # Call vLLM chat endpoint with image
        response = await self._call_vision_endpoint(
            endpoint="/v1/chat/completions",
            image=processed_image,
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )

        return response["choices"][0]["message"]["content"]
```

### Key Technical Decisions

**1. Vision Model Selection Strategy:**

**Vision Model Comparison and Selection Guide:**

| Model | Best For | Performance | Quality | Resource Usage | Recommended Use Case |
|-------|----------|-------------|---------|----------------|---------------------|
| **LLaVA-1.6-Mistral-7B** | General purpose | Fast (1-2s/image) | High | Medium (7B params) | **Default choice** - Best balance of speed and quality |
| **LLaVA-1.6-Vicuna-13B** | High accuracy | Medium (2-3s/image) | Very High | High (13B params) | Production deployments prioritizing quality |
| **Qwen-VL-Chat** | Multilingual OCR | Medium (2-3s/image) | Very High (OCR) | Medium (7B params) | Documentation with non-English text |
| **Phi-3-Vision-128K** | Low latency | Very Fast (0.5-1s/image) | Good | Low (4B params) | High-throughput crawling, cost optimization |
| **CogVLM** | Technical diagrams | Slow (3-5s/image) | Excellent | Very High (17B params) | Complex architecture diagrams, detailed analysis |

**Selection Logic:**
```python
def select_vision_model(
    use_case: str,
    priority: str = "balanced"  # or "speed", "quality", "cost"
) -> str:
    """
    Automatically select best vision model based on use case and priority

    Use cases:
    - general: Mixed content (screenshots, diagrams, photos)
    - technical: Technical documentation, architecture diagrams
    - multilingual: Non-English documentation
    - high_volume: Large-scale crawling with thousands of images

    Priorities:
    - speed: Minimize latency (Phi-3-Vision)
    - quality: Maximize accuracy (LLaVA-1.6-Vicuna-13B or CogVLM)
    - balanced: Good speed and quality (LLaVA-1.6-Mistral-7B)
    - cost: Minimize compute (Phi-3-Vision)
    """
    if priority == "speed" or use_case == "high_volume":
        return "microsoft/Phi-3-vision-128k-instruct"

    if priority == "quality" and use_case == "technical":
        return "THUDM/cogvlm-chat-hf"

    if use_case == "multilingual":
        return "Qwen/Qwen-VL-Chat"

    # Default: balanced performance
    return "llava-hf/llava-v1.6-mistral-7b-hf"
```

**Model Capabilities Matrix:**

| Capability | LLaVA-1.6 | Qwen-VL | Phi-3-Vision | CogVLM |
|------------|-----------|---------|--------------|---------|
| Image Captioning | ✅✅✅ | ✅✅✅ | ✅✅ | ✅✅✅✅ |
| OCR (English) | ✅✅ | ✅✅✅ | ✅✅ | ✅✅✅ |
| OCR (Multilingual) | ✅ | ✅✅✅✅ | ✅ | ✅✅ |
| Diagram Understanding | ✅✅ | ✅✅✅ | ✅✅ | ✅✅✅✅ |
| Code Screenshot Recognition | ✅✅✅ | ✅✅ | ✅✅ | ✅✅✅ |
| Embedding Generation | ✅✅✅ | ✅✅✅ | ✅✅✅ | ✅✅ |

**Fallback Strategy:**
- Primary: vLLM-hosted vision models (configured model)
- Fallback 1: Alternative vLLM model (if primary unavailable)
- Fallback 2: OpenAI GPT-4V (if vLLM unavailable and OpenAI configured)
- Emergency fallback: Metadata-only indexing (alt text, surrounding context)

**2. Image Preprocessing:**
- Resize large images to max 1024x1024 to reduce processing time
- Convert to RGB format (remove alpha channel)
- Compress JPEG to reduce bandwidth
- Cache preprocessed images to avoid redundant processing

**Preprocessing Cache Strategy:**
```python
from functools import lru_cache
import hashlib

class ImagePreprocessingCache:
    """Cache preprocessed images to avoid redundant work"""

    def __init__(self, max_size_mb: int = 100):
        self.cache = {}
        self.max_size = max_size_mb * 1024 * 1024  # Convert to bytes
        self.current_size = 0

    def get_cache_key(self, image_data: bytes) -> str:
        """Generate cache key from image content hash"""
        return hashlib.md5(image_data).hexdigest()

    def get(self, image_data: bytes) -> Optional[bytes]:
        """Retrieve cached preprocessed image"""
        cache_key = self.get_cache_key(image_data)
        return self.cache.get(cache_key)

    def put(self, original_data: bytes, preprocessed_data: bytes):
        """Store preprocessed image in cache with size limit"""
        cache_key = self.get_cache_key(original_data)

        # Evict oldest entries if cache full
        if self.current_size + len(preprocessed_data) > self.max_size:
            self._evict_oldest()

        self.cache[cache_key] = preprocessed_data
        self.current_size += len(preprocessed_data)

# Global cache instance
preprocessing_cache = ImagePreprocessingCache()
```

**3. Caption Quality:**
- Use structured prompts for different image types
- Validate caption length (min 10 chars, max 500 chars)
- Fallback to alt text if caption is low quality or empty
- Store confidence score if available from vision model

**4. OCR Strategy:**

**Primary Approach: Vision Model Native OCR (Story 2.3 Scope)**
- Use vision model's native text extraction capabilities via `extract_text_from_image()`
- Best integration with vLLM vision models (no additional dependencies)
- Leverages model's understanding of context and layout
- Consistent with other vision operations

**OCR Implementation with Vision Models:**
```python
async def extract_text_from_image(
    self,
    image_data: bytes,
    model: str = None
) -> str:
    """Extract text from image using vision model OCR"""
    model = model or self.config.vision_model

    # Use vision model with OCR-focused prompt
    prompt = (
        "Extract all visible text from this image. "
        "Preserve formatting and structure. "
        "If no text is visible, respond with 'No text detected'."
    )

    response = await self._call_vision_endpoint(
        endpoint="/v1/chat/completions",
        image=self._preprocess_image(image_data),
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )

    extracted_text = response["choices"][0]["message"]["content"]

    # Filter out "no text" responses
    if "no text" in extracted_text.lower():
        return ""

    return extracted_text
```

**Future Enhancement: Tesseract OCR Fallback (Out of Scope for Story 2.3)**
- **Not included in Story 2.3 implementation**
- Potential future enhancement for specialized OCR needs
- Would require additional dependencies (pytesseract, tesseract-ocr)
- Recommended for future story if vision model OCR quality insufficient

**OCR Quality Control:**
- Validate extracted text is meaningful (length > 5 chars)
- Filter out common OCR artifacts ("|||", "===", random characters)
- Combine OCR text with image context for better search
- Log OCR confidence/quality metrics if available from model

**5. Performance Optimization and Batch Processing:**

**Batch Processing Coordination:**

When a page contains multiple images, coordinate vision processing to optimize throughput and resource usage:

```python
import asyncio
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class ImageProcessingTask:
    """Represents a single image to process"""
    image_id: str
    image_data: bytes
    alt_text: str
    context: str

@dataclass
class ImageProcessingResult:
    """Result of vision model processing"""
    image_id: str
    embedding: List[float]
    description: str
    extracted_text: str
    processing_time_ms: float
    status: str  # success, partial, failed

async def process_images_batch(
    images: List[ImageProcessingTask],
    vision_provider: VisionProvider,
    concurrency_limit: int = 3
) -> List[ImageProcessingResult]:
    """
    Process multiple images with controlled concurrency

    Features:
    - Parallel processing with semaphore-based concurrency control
    - Progress tracking for long-running operations
    - Graceful error handling (continue on individual failures)
    - Memory management (process in batches if too many images)
    - Performance metrics collection

    Args:
        images: List of images to process
        vision_provider: Vision model provider
        concurrency_limit: Max concurrent vision model calls (default: 3)

    Returns:
        List of processing results (same order as input)
    """
    semaphore = asyncio.Semaphore(concurrency_limit)
    results = []

    async def process_with_semaphore(task: ImageProcessingTask) -> ImageProcessingResult:
        """Process single image with semaphore for concurrency control"""
        async with semaphore:
            start_time = asyncio.get_event_loop().time()

            try:
                # Generate embedding
                embedding = await vision_provider.generate_image_embedding(
                    task.image_data
                )

                # Generate caption
                description = await vision_provider.generate_image_caption(
                    task.image_data
                )

                # Extract text via OCR
                extracted_text = await vision_provider.extract_text_from_image(
                    task.image_data
                )

                processing_time = (asyncio.get_event_loop().time() - start_time) * 1000

                return ImageProcessingResult(
                    image_id=task.image_id,
                    embedding=embedding,
                    description=description or task.alt_text,
                    extracted_text=extracted_text or "",
                    processing_time_ms=processing_time,
                    status="success"
                )

            except Exception as e:
                logger.warning(f"Vision processing failed for {task.image_id}: {e}")
                processing_time = (asyncio.get_event_loop().time() - start_time) * 1000

                # Return partial result with fallback data
                return ImageProcessingResult(
                    image_id=task.image_id,
                    embedding=None,
                    description=task.alt_text or "No description available",
                    extracted_text="",
                    processing_time_ms=processing_time,
                    status="failed"
                )

    # Process all images concurrently with controlled concurrency
    logger.info(f"Processing {len(images)} images with concurrency={concurrency_limit}")

    # Create tasks for all images
    tasks = [process_with_semaphore(img) for img in images]

    # Execute with progress tracking
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Filter out exceptions (already logged in process_with_semaphore)
    valid_results = [r for r in results if isinstance(r, ImageProcessingResult)]

    # Log batch statistics
    success_count = sum(1 for r in valid_results if r.status == "success")
    avg_time = sum(r.processing_time_ms for r in valid_results) / len(valid_results) if valid_results else 0

    logger.info(
        f"Batch processing complete: {success_count}/{len(images)} successful, "
        f"avg time: {avg_time:.1f}ms"
    )

    return valid_results

# Example usage in crawl pipeline:
async def crawl_with_vision_processing(url: str):
    """Enhanced crawl with vision processing"""
    # Extract images (Story 2.2)
    page_images = await extract_images_from_page(url)

    # Create processing tasks
    tasks = [
        ImageProcessingTask(
            image_id=img.id,
            image_data=img.data,
            alt_text=img.alt_text,
            context=img.surrounding_context
        )
        for img in page_images
    ]

    # Batch process with vision model
    results = await process_images_batch(
        tasks,
        vision_provider,
        concurrency_limit=int(os.getenv("VISION_MODEL_CONCURRENCY", "3"))
    )

    # Store results in database
    for result in results:
        await db.update_image_vision_data(
            result.image_id,
            embedding=result.embedding,
            description=result.description,
            extracted_text=result.extracted_text
        )
```

**Performance Optimization Strategies:**

- **Concurrency Control**: Limit parallel vision calls to prevent overwhelming the endpoint
  - Default: 3 concurrent calls
  - Configurable via `VISION_MODEL_CONCURRENCY` environment variable
  - Adjust based on vLLM deployment capacity

- **Memory Management**: For pages with many images (>20), process in sub-batches
  ```python
  MAX_BATCH_SIZE = 20
  for i in range(0, len(images), MAX_BATCH_SIZE):
      batch = images[i:i + MAX_BATCH_SIZE]
      results = await process_images_batch(batch, vision_provider)
  ```

- **Timeout Handling**: Individual image timeouts don't block entire batch
  - Per-image timeout: 30 seconds (configurable)
  - Graceful degradation on timeout

- **Cache Coordination**: Leverage deduplication from Story 2.2
  - Skip vision processing for duplicate images
  - Reuse embeddings from previous crawls

- **Progress Tracking**: Log progress for long-running vision operations
  ```python
  logger.info(f"Vision processing: {completed}/{total} images processed ({progress}%)")
  ```

### Configuration Examples

```bash
# Vision Model Configuration (from Story 2.1)
AI_PROVIDER=vllm
VLLM_VISION_MODEL=llava-hf/llava-v1.6-mistral-7b-hf

# Vision Processing Flags
GENERATE_IMAGE_CAPTIONS=true
EXTRACT_IMAGE_TEXT=true  # OCR

# Caption Generation
IMAGE_CAPTION_PROMPT_DIAGRAM="Describe this technical diagram, focusing on architecture and data flow"
IMAGE_CAPTION_PROMPT_SCREENSHOT="Describe what is shown in this screenshot, including UI elements and actions"
IMAGE_CAPTION_PROMPT_DEFAULT="Describe this image in detail"

# Performance Tuning
VISION_MODEL_TIMEOUT=30  # Seconds
VISION_MODEL_MAX_RETRIES=3
VISION_MODEL_CONCURRENCY=3  # Max parallel vision model calls
IMAGE_PREPROCESSING_MAX_SIZE=1024  # Max width/height in pixels

# Quality Control
IMAGE_CAPTION_MIN_LENGTH=10  # Minimum caption length
IMAGE_CAPTION_MAX_LENGTH=500
OCR_MIN_CONFIDENCE=0.5  # Minimum OCR confidence to include text

# Graceful Degradation
VISION_MODEL_REQUIRED=false  # If true, fail crawl on vision error
VISION_MODEL_FALLBACK_TO_ALT_TEXT=true
```

### Implementation Guidelines

**Image Preprocessing Pipeline:**
```python
def _preprocess_image(image_data: bytes) -> bytes:
    """
    Preprocess image for vision model

    Steps:
    1. Open image with PIL
    2. Convert to RGB (remove alpha)
    3. Resize if larger than max dimensions
    4. Compress to JPEG format
    5. Return bytes
    """
    from PIL import Image
    import io

    # Open image
    img = Image.open(io.BytesIO(image_data))

    # Convert to RGB
    if img.mode != 'RGB':
        img = img.convert('RGB')

    # Resize if needed
    max_size = (1024, 1024)
    if img.size[0] > max_size[0] or img.size[1] > max_size[1]:
        img.thumbnail(max_size, Image.Resampling.LANCZOS)

    # Compress to JPEG
    output = io.BytesIO()
    img.save(output, format='JPEG', quality=85)
    return output.getvalue()
```

**Caption Quality Validation:**
```python
def _validate_caption(caption: str, min_length: int = 10) -> bool:
    """
    Validate caption quality

    Checks:
    - Not empty
    - Minimum length
    - Not generic/low-quality ("An image", "A picture")
    - Contains meaningful content
    """
    if not caption or len(caption) < min_length:
        return False

    # Check for generic captions
    generic_phrases = ["an image", "a picture", "a photo", "i'm sorry"]
    caption_lower = caption.lower()
    if any(phrase in caption_lower for phrase in generic_phrases):
        return False

    return True
```

**Error Handling Strategy:**
```python
async def process_image_with_vision(
    image_data: bytes,
    vision_provider: VisionProvider,
    fallback_alt_text: str = None
) -> dict:
    """
    Process image with graceful degradation

    Returns:
    {
        "embedding": [...],
        "description": "...",
        "extracted_text": "...",
        "processing_status": "success|partial|failed"
    }
    """
    result = {
        "embedding": None,
        "description": None,
        "extracted_text": None,
        "processing_status": "failed"
    }

    try:
        # Try to generate embedding
        result["embedding"] = await vision_provider.generate_image_embedding(
            image_data
        )
    except Exception as e:
        logger.warning(f"Image embedding generation failed: {e}")
        # Continue with other processing

    try:
        # Try to generate caption
        caption = await vision_provider.generate_image_caption(image_data)
        if _validate_caption(caption):
            result["description"] = caption
        else:
            # Fallback to alt text
            result["description"] = fallback_alt_text or "No description available"
    except Exception as e:
        logger.warning(f"Image caption generation failed: {e}")
        result["description"] = fallback_alt_text or "No description available"

    try:
        # Try OCR
        result["extracted_text"] = await vision_provider.extract_text_from_image(
            image_data
        )
    except Exception as e:
        logger.warning(f"OCR text extraction failed: {e}")
        # Continue without OCR text

    # Determine processing status
    if result["embedding"] and result["description"]:
        result["processing_status"] = "success"
    elif result["embedding"] or result["description"]:
        result["processing_status"] = "partial"

    return result
```

### Testing

#### Testing Standards
- **Test file location:** `tests/test_vision_model_integration.py`
- **Test frameworks:** pytest with asyncio support
- **Test data:** Mock images (small JPEG/PNG files)
- **Mocking:** Mock vLLM API responses
- **Coverage target:** 85%+ for vision model code

#### Test Scenarios

1. **Image Embedding Generation:**
   - Successful embedding generation
   - Batch embedding generation for multiple images
   - Handle large images (preprocessing)
   - Handle invalid image data
   - Timeout handling
   - Retry logic on transient errors

2. **Caption Generation:**
   - Generate captions for different image types
   - Use different prompt templates
   - Validate caption quality
   - Fallback to alt text on failure
   - Handle empty/low-quality captions

3. **OCR Text Extraction:**
   - Extract text from screenshots
   - Extract text from diagrams
   - Handle images without text
   - Handle multi-language text
   - Filter low-confidence results

4. **Integration with Embedding Pipeline:**
   - Create image embeddings via `create_image_embedding()`
   - Provider selection (vLLM, OpenAI fallback)
   - Batch processing
   - Backward compatibility with text embeddings

5. **Configuration:**
   - Enable/disable caption generation
   - Enable/disable OCR
   - Configure timeouts and retries
   - Configure quality thresholds

6. **Graceful Degradation:**
   - Vision model unavailable → metadata-only indexing
   - Partial failure (embedding works, caption fails)
   - Circuit breaker after repeated failures
   - Fallback to alt text

7. **Performance:**
   - Measure vision model call latency
   - Verify < 2s per image (p95)
   - Test parallel processing
   - Test timeout enforcement

8. **Database Integration:**
   - Store embeddings in vector database
   - Store descriptions and OCR text
   - Vector similarity search for images
   - Test with multiple database providers

#### Example Test Structure

```python
import pytest
from src.ai_providers.providers.vllm_provider import VLLMProvider
from src.utils_refactored import create_image_embedding

@pytest.mark.asyncio
async def test_image_embedding_generation():
    """Test generating embeddings for images"""
    # Mock vLLM response
    # Create small test image
    # Generate embedding
    # Verify dimensions and format

@pytest.mark.asyncio
async def test_image_caption_generation():
    """Test generating captions for images"""
    # Mock vLLM response
    # Create test image
    # Generate caption
    # Verify caption quality

@pytest.mark.asyncio
async def test_ocr_text_extraction():
    """Test extracting text from images"""
    # Mock vLLM response
    # Create image with text (screenshot)
    # Extract text
    # Verify text accuracy

@pytest.mark.asyncio
async def test_graceful_degradation_on_vision_failure():
    """Test fallback when vision model fails"""
    # Mock vision model failure
    # Process image
    # Verify fallback to metadata-only indexing
    # Verify crawl continues

@pytest.mark.asyncio
async def test_vision_processing_performance():
    """Test vision model performance"""
    # Create test image
    # Measure processing time
    # Verify < 2s latency
    # Test parallel processing

@pytest.mark.asyncio
async def test_image_embedding_database_storage():
    """Test storing image embeddings in database"""
    # Generate image embedding
    # Store in database
    # Retrieve via vector similarity search
    # Verify results
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-01 | 1.0 | Initial story creation for vision model integration | Sarah (PO) |
| 2025-10-01 | 1.1 | Added vision model selection guide, batch processing coordination, preprocessing cache, and OCR clarification | Sarah (PO) |

## Dev Agent Record

### Agent Model Used
_(To be populated by dev agent during implementation)_

### Debug Log References
_(To be populated by dev agent during implementation)_

### Completion Notes List
_(To be populated by dev agent during implementation)_

### File List
**Expected Files to be Created/Modified:**
- `src/ai_providers/base.py` (modified - add VisionProvider interface)
- `src/ai_providers/providers/vllm_provider.py` (modified - implement vision methods)
- `src/utils_refactored.py` (modified - add image embedding functions)
- `src/crawl4ai_mcp.py` (modified - integrate vision processing)
- `src/vision/` (new directory for vision utilities)
- `src/vision/preprocessing.py` (new - image preprocessing)
- `src/vision/prompts.py` (new - caption prompt templates)
- `tests/test_vision_model_integration.py` (new)
- `tests/fixtures/test_images/` (new - test image data)
- `.env.example` (modified - add vision config)
- `CLAUDE.md` (modified - add vision model docs)

## QA Results
_(To be populated by QA agent after implementation)_
