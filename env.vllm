# vLLM Docker Compose Environment Configuration
# Optimized for 5x RTX 3090 (24GB each) + 256GB RAM

# =============================================================================
# Hardware Configuration
# =============================================================================
# GPU Allocation:
# - GPUs 0-1: Text LLM (2x RTX 3090, 48GB total)
# - GPU 2:    Embedding (1x RTX 3090, 24GB)
# - GPUs 3-4: Vision (2x RTX 3090, 48GB total)
# - Total: 5x RTX 3090, 120GB VRAM
# - System RAM: 256GB

# =============================================================================
# vLLM Server Configuration
# =============================================================================

# Text LLM Configuration (GPUs 0-1)
VLLM_TEXT_MODEL=meta-llama/Llama-3.1-8B-Instruct
VLLM_TEXT_GPU_MEMORY=0.85
VLLM_TEXT_MAX_LEN=16384
VLLM_TEXT_GPUS=0,1
VLLM_TEXT_TENSOR_PARALLEL=2

# Embedding Configuration (GPU 2)
VLLM_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
VLLM_EMBEDDING_GPU_MEMORY=0.8
VLLM_EMBEDDING_DIMENSIONS=1024
VLLM_EMBEDDING_GPUS=2
VLLM_EMBEDDING_TENSOR_PARALLEL=1

# Vision Configuration (GPUs 3-4)
VLLM_VISION_MODEL=llava-hf/llava-v1.6-mistral-7b-hf
VLLM_VISION_GPU_MEMORY=0.8
VLLM_VISION_ENABLED=true
VLLM_VISION_GPUS=3,4
VLLM_VISION_TENSOR_PARALLEL=2

# =============================================================================
# Server Ports
# =============================================================================
VLLM_TEXT_PORT=8000
VLLM_EMBEDDING_PORT=8001
VLLM_VISION_PORT=8002
NGINX_PROXY_PORT=8080

# =============================================================================
# Monitoring Ports
# =============================================================================
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000

# =============================================================================
# GPU Configuration
# =============================================================================
# GPU allocation per service
TEXT_GPUS=0,1
EMBEDDING_GPUS=2
VISION_GPUS=3,4

# =============================================================================
# Model Storage
# =============================================================================
MODELS_PATH=./models
LOGS_PATH=./logs

# =============================================================================
# Performance Tuning (Optimized for RTX 3090)
# =============================================================================
VLLM_DTYPE=float16
VLLM_DISABLE_LOG_REQUESTS=true

# Text LLM Performance
VLLM_TEXT_MAX_BATCHED_TOKENS=8192
VLLM_TEXT_MAX_NUM_SEQS=256

# Embedding Performance
VLLM_EMBEDDING_MAX_BATCHED_TOKENS=16384
VLLM_EMBEDDING_MAX_NUM_SEQS=512

# Vision Performance
VLLM_VISION_MAX_BATCHED_TOKENS=4096
VLLM_VISION_MAX_NUM_SEQS=128

# =============================================================================
# Health Check Configuration
# =============================================================================
HEALTH_CHECK_INTERVAL=30s
HEALTH_CHECK_TIMEOUT=10s
HEALTH_CHECK_RETRIES=3
HEALTH_CHECK_START_PERIOD=180s

# =============================================================================
# Security (Optional)
# =============================================================================
# VLLM_API_KEY=your-secret-api-key
# VLLM_ENABLE_AUTH=false

# =============================================================================
# Logging
# =============================================================================
LOG_LEVEL=INFO
LOG_FORMAT=json

# =============================================================================
# Advanced GPU Settings
# =============================================================================
# Enable GPU memory optimization
GPU_MEMORY_FRACTION=0.9
CUDA_LAUNCH_BLOCKING=0

# =============================================================================
# Model Download Settings
# =============================================================================
# Pre-download models for faster startup
DOWNLOAD_MODELS=true
MODEL_CACHE_DIR=/models/cache
